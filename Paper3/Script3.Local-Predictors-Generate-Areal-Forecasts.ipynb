{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries for data analysis\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# specialized libraries\n",
    "import xagg as xa # spatial aggregation of data taken into consideration overlap of grid cells to shapefile\n",
    "\n",
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_precip = ''\n",
    "input_dir_wvf = ''\n",
    "input_dir_rh = ''\n",
    "output_data_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warn_areas_Calabria = gpd.read_file('zip://../Shapefiles/Calabria/WarningAreas.zip')\n",
    "warn_areas_Calabria = warn_areas_Calabria.drop(columns=['id']) # keep only columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final format of shapefiles and data needed per analysed area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepros_shpf(shapefile_input):\n",
    "    \n",
    "    output_shp = shapefile_input.to_crs(\"EPSG:4326\") # convert to lat/lon coordinate system \n",
    "    output_shp.columns = ['name_area', 'geometry'] # change names of columns for consistency\n",
    "\n",
    "    output_shp['Area'] = len(output_shp)\n",
    "    areal_full = output_shp.dissolve(by='Area')\n",
    "    areal_full.name_area = 'Full'\n",
    "    output_shp = pd.concat([output_shp, areal_full]).drop(columns='Area')\n",
    "    \n",
    "    # find boundary for gridded data based on the warning areas (& add 0.1 degree extra boundary for security)\n",
    "    total_bounds = output_shp.total_bounds\n",
    "    total_bounds = [np.floor(i) if count in [0, 1] else np.ceil(i) for count, i in enumerate(total_bounds)]\n",
    "    \n",
    "    return [output_shp, total_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dictionary of names with the subdomains used and their shapefiles in geopandas dataframes\n",
    "domains_used = {\n",
    "                'Calabria': prepros_shpf(warn_areas_Calabria),\n",
    "                }\n",
    "\n",
    "del(warn_areas_Calabria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for spatial aggregation & extremes identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_subset(data_full, dom_name_used):\n",
    "    \n",
    "    bbox_used = domains_used[dom_name_used][1]\n",
    "    dt_sbst = data_full.sel(longitude=slice(bbox_used[0], bbox_used[2]), \n",
    "                            latitude=slice(bbox_used[3], bbox_used[1]))\n",
    "    \n",
    "    return dt_sbst    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_aggreg(input_data):\n",
    "    \n",
    "    data_xr_used, domain_name_used = input_data # data used    \n",
    "    \n",
    "    # create auxiliary data for the domain of interest\n",
    "    warn_areas = domains_used[domain_name_used][0]\n",
    "    data_xr_final = generate_data_subset(data_xr_used, domain_name_used)\n",
    "    \n",
    "    weightmap = xa.pixel_overlaps(data_xr_final, warn_areas) # overlap of pixels & polygons\n",
    "    \n",
    "    aggregated = xa.aggregate(data_xr_final, weightmap) # calculation of areal average\n",
    "    aggregated = aggregated.to_dataset()[data_xr_used.name] # convert to dataarray\n",
    "    aggregated = aggregated.rename({'pix_idx': 'WarnArea'}) # rename coordinate\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_aggregation_forecasts(dataset_used):\n",
    "    with suppress_stdout():\n",
    "        spat_aggr = [spatial_aggreg([dataset_used, i_area]) for i_area in domains_used]\n",
    "        \n",
    "    return spat_aggr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECMWF Reforecsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dates that have the same Cycle so that the forecast data are consistent. Details about changes in cycles are available at https://www.ecmwf.int/en/forecasts/documentation-and-support/changes-ecmwf-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dates for Cycle 46r1 11 June 2019 - 30 June 2020\n",
    "start_date = '20190611'\n",
    "end_date = '20200630'\n",
    "\n",
    "in_dates = pd.date_range(start_date, end_date)\n",
    "\n",
    "# keep Mondays (0) and Thursdays (3)\n",
    "kept_dates = (in_dates.weekday == 0) | (in_dates.weekday == 3)\n",
    "in_dates = in_dates[kept_dates]\n",
    "in_dates = in_dates.strftime('%Y%m%d')\n",
    "\n",
    "del(start_date, end_date, kept_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_precip_init_date(init_date_used):\n",
    "    \n",
    "    ' Get the reforecast data for the selected initialization date '\n",
    "    ' There is no need to derive ens mean for precipitation data, cause the mean is very biased and wrong! '\n",
    "    \n",
    "    # get the data of the control member (cf)\n",
    "    file_name = input_dir_precip+'cf/Precipitation_cf_'+init_date_used+'.grb'\n",
    "    control_forecast = xr.open_dataarray(file_name, engine='cfgrib')\n",
    "    control_forecast = control_forecast.assign_coords({'number': 0})\n",
    "    \n",
    "    # get the data of the ensemble members (pf)\n",
    "    file_name = input_dir_precip+'pf/Precipitation_pf_'+init_date_used+'.grb'\n",
    "    ensemble_forecast = xr.open_dataarray(file_name, engine='cfgrib')\n",
    "    \n",
    "    final = xr.concat([control_forecast, ensemble_forecast], dim='number') # combine cf and pf data\n",
    "    \n",
    "    # Precip is a cumulative variable, so for daily values we need differences of next with day of interest\n",
    "    final = xr.concat([final.isel(step=0), final.diff('step')], dim='step') # no +00 lead time is available\n",
    "    \n",
    "    # actual step is previous days, since precip is cumulative, so it gives data till the end date of interest\n",
    "    final = final.assign_coords({'step': final.step.values-np.timedelta64(1, 'D')})\n",
    "    \n",
    "    final = final.where(final>0) # mask values that are <=0 (it happens due to interpolations)\n",
    "    final = final.fillna(0) # replace NaNs (were previously negative or 0) with 0\n",
    "    final = final.reset_coords(drop=True)\n",
    "    \n",
    "    aggr_data = spatial_aggregation_forecasts(final)\n",
    "    \n",
    "    del(control_forecast, ensemble_forecast, final)\n",
    "    \n",
    "    return aggr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 110/110 [09:19<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "prec_data = list(tqdm.tqdm(pool.imap(frcst_precip_init_date, in_dates), \n",
    "                           total=len(in_dates), position=0, leave=True))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "precip_final = {}\n",
    "for i_ind, i_area in enumerate(domains_used):\n",
    "    i_precip = [i_pr[i_ind] for i_pr in prec_data]\n",
    "    precip_final[i_area] = xr.concat(i_precip, dim='time')\n",
    "    precip_final[i_area].to_netcdf(f'{output_data_file}{i_area}_precip.nc')\n",
    "    \n",
    "del(pool, prec_data, i_ind, i_area, i_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_wvf_init_date(init_date_used):\n",
    "    \n",
    "    ' Get the reforecast data for the selected initialization date '\n",
    "    ' There is no need to derive ens mean, cause the mean is very biased and wrong! '\n",
    "    \n",
    "    # get the data of the control member (cf)\n",
    "    file_name = f'{input_dir_wvf}CF/CF_{init_date_used}.grb'\n",
    "    control_forecast = xr.open_dataset(file_name, engine='cfgrib')\n",
    "    control_forecast = control_forecast.assign_coords({'number': 0})\n",
    "\n",
    "    # get the data of the ensemble members (pf)\n",
    "    file_name = f'{input_dir_wvf}PF/PF_{init_date_used}.grb'\n",
    "    ensemble_forecast = xr.open_dataset(file_name, engine='cfgrib')\n",
    "\n",
    "    final = xr.concat([control_forecast, ensemble_forecast], dim='number') # combine cf and pf data\n",
    "    final = final.reset_coords(drop=True)\n",
    "    final  = final.to_array().rename({'variable': 'direction'}) # dot (line 21) does not work in dataset\n",
    "\n",
    "    # get weighted 6-hourly average WVF\n",
    "    weight = xr.DataArray(np.array([0.5]+[1]*3+[0.5]), dims=['window'])\n",
    "    final_daily = final.rolling(step=5, center=True).construct('window').dot(weight)/weight.sum()\n",
    "    step_daily_format = final_daily.step.values/np.timedelta64(1, 'D')\n",
    "    \n",
    "    final_daily = final_daily.isel(step=(step_daily_format%1==0.5)) # keep only 12 UTC valid times\n",
    "    final_steps = (final_daily.step-np.timedelta64(12, 'h')) # step as 0UTC for each day\n",
    "    final_daily = final_daily.assign_coords({'step': final_steps})\n",
    "    \n",
    "    del(control_forecast, ensemble_forecast)\n",
    "    \n",
    "    return final_daily.to_dataset('direction') # convert back to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_wvf_all_init_date(init_date_used):\n",
    "    \n",
    "    wvf_directions = frcst_wvf_init_date(init_date_used)\n",
    "    wvf_northwards = wvf_directions['viwvn']\n",
    "    wvf_northwards.name = 'wvf'\n",
    "    wvf_eastwards = wvf_directions['viwve']\n",
    "    wvf_eastwards.name = 'wvf'\n",
    "    wvf_total = np.sqrt(wvf_northwards**2 + wvf_eastwards**2)\n",
    "    \n",
    "    wvf_northwards = spatial_aggregation_forecasts(wvf_northwards)\n",
    "    wvf_eastwards = spatial_aggregation_forecasts(wvf_eastwards)\n",
    "    wvf_total = spatial_aggregation_forecasts(wvf_total)\n",
    "    \n",
    "    wvf_name = pd.Index(['NorthW', 'SouthW', 'EastW', 'WestW', 'Total_Grid', 'Total_Area'], name='wvf_direction')\n",
    "    \n",
    "    aggr_data = []\n",
    "    for i_dom in range(len(domains_used)):\n",
    "        i_dom_northw = wvf_northwards[i_dom]\n",
    "        i_dom_southw = -wvf_northwards[i_dom]\n",
    "        i_dom_eastw = wvf_eastwards[i_dom]\n",
    "        i_dom_westw = -wvf_eastwards[i_dom]\n",
    "        i_dom_total_grid = wvf_total[i_dom]\n",
    "        i_dom_total_area = np.sqrt(i_dom_northw**2 + i_dom_eastw**2)\n",
    "        i_dom_final = [i_dom_northw, i_dom_southw, i_dom_eastw, i_dom_westw, i_dom_total_grid, i_dom_total_area]\n",
    "        i_dom_final = xr.concat(i_dom_final, dim=wvf_name)\n",
    "        aggr_data.append(i_dom_final)\n",
    "    \n",
    "    del(wvf_directions, wvf_northwards, wvf_eastwards, wvf_total, i_dom_final,\n",
    "        i_dom_northw, i_dom_southw, i_dom_eastw, i_dom_westw, i_dom_total_grid, i_dom_total_area)\n",
    "    \n",
    "    return aggr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "wvf_data = list(tqdm.tqdm(pool.imap(frcst_wvf_all_init_date, in_dates[:]), \n",
    "                          total=len(in_dates), position=0, leave=True))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "wvf_final = {}\n",
    "for i_ind, i_area in enumerate(domains_used):\n",
    "    i_wvf = [i_pr[i_ind] for i_pr in wvf_data]\n",
    "    wvf_final[i_area] = xr.concat(i_wvf, dim='time')\n",
    "    wvf_final[i_area].to_netcdf(f'{output_data_file}{i_area}_wvf.nc')\n",
    "    \n",
    "del(pool, wvf_data, i_ind, i_area, i_wvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relhum(data_used):\n",
    "    \n",
    "    temp = data_used['t']\n",
    "    sphm = data_used['q']\n",
    "    relhum = .263*temp.isobaricInhPa*100*sphm/np.e**(17.67*(temp-274.16)/(temp-29.65))\n",
    "    relhum = relhum.rename({'isobaricInhPa': 'pressure_level'})\n",
    "    \n",
    "    return relhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_relhum_init_date(init_date_used):\n",
    "    \n",
    "    ' Get the reforecast data for the selected initialization date '\n",
    "    ' There is no need to derive ens mean, cause the mean is very biased and wrong! '\n",
    "    \n",
    "    # get the data of the control member (cf)\n",
    "    file_name = f'{input_dir_rh}CF/CF_{init_date_used}.grb'\n",
    "    control_forecast = xr.open_dataset(file_name, engine='cfgrib')\n",
    "    control_forecast = control_forecast.assign_coords({'number': 0})\n",
    "    \n",
    "    # get the data of the ensemble members (pf)\n",
    "    file_name = f'{input_dir_rh}PF/PF_{init_date_used}.grb'\n",
    "    ensemble_forecast = xr.open_dataset(file_name, engine='cfgrib')\n",
    "    \n",
    "    final = xr.concat([control_forecast, ensemble_forecast], dim='number') # combine cf and pf data\n",
    "    \n",
    "    final = calc_relhum(final.reset_coords(drop=True)) # calculate rel hum based on available temp and spe hum\n",
    "\n",
    "    # calculate daily rh of 850 (all steps available) and 700 (includes only steps 0, 12, 24, ...)\n",
    "    weight850 = xr.DataArray(np.array([0.5]+[1]*3+[0.5]), dims=['window'])\n",
    "    rh850 = final.sel(pressure_level=850).copy(deep=True)\n",
    "\n",
    "    weight700 = xr.DataArray(np.array([0.5]+[1]+[0.5]), dims=['window'])\n",
    "    rh700 = final.sel(pressure_level=700).dropna('step').copy(deep=True)\n",
    "    \n",
    "\n",
    "    final_daily850 = rh850.rolling(step=5, center=True).construct('window').dot(weight850)/weight850.sum()\n",
    "    step_daily850_format = final_daily850.step.values/np.timedelta64(1, 'D')\n",
    "    final_daily850 = final_daily850.isel(step=(step_daily850_format%1==0.5)) # keep only 12 UTC valid times\n",
    "\n",
    "    final_daily700 = rh700.rolling(step=3, center=True).construct('window').dot(weight700)/weight700.sum()\n",
    "    step_daily700_format = final_daily700.step.values/np.timedelta64(1, 'D')\n",
    "    final_daily700 = final_daily700.isel(step=(step_daily700_format%1==0.5)) # keep only 12 UTC valid times\n",
    "\n",
    "    final_daily = xr.concat([final_daily700, final_daily850], dim='pressure_level')\n",
    "    final_steps = (final_daily.step-np.timedelta64(12, 'h')) # step as 0UTC for each day\n",
    "    final_daily = final_daily.assign_coords({'step': final_steps})\n",
    "    final_daily.name = 'relhum'\n",
    "    \n",
    "    aggr_data = spatial_aggregation_forecasts(final_daily)\n",
    "    \n",
    "    del(control_forecast, ensemble_forecast, final, rh850, rh700, final_daily850, final_daily700, final_daily)\n",
    "    \n",
    "    return aggr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes=5) # object for multiprocessing\n",
    "rlhm_data = list(tqdm.tqdm(pool.imap(frcst_relhum_init_date, in_dates), \n",
    "                           total=len(in_dates), position=0, leave=True))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "relhum_final = {}\n",
    "for i_ind, i_area in enumerate(domains_used):\n",
    "    i_relhum = [i_pr[i_ind] for i_pr in rlhm_data]\n",
    "    relhum_final[i_area] = xr.concat(i_relhum, dim='time')\n",
    "    relhum_final[i_area].to_netcdf(f'{output_data_file}{i_area}_relhum.nc')\n",
    "    \n",
    "del(pool, rlhm_data, i_ind, i_area, i_relhum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
