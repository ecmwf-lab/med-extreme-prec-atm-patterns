{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries for data analysis\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = ''\n",
    "\n",
    "season_studied = 'Winter' # All, Winter or Summer!\n",
    "observational_dataset = 'EOBS' # ERA5 or EOBS\n",
    "rolling_days = 3 # what temporal resolution to analyse? in number of days; integer>=1\n",
    "frcst_med = out_dir+'Med_LocalizedPatterns_ForecastAllocations.nc'\n",
    "\n",
    "bootstr = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_name = f'{observational_dataset}_{season_studied}_Days{rolling_days}'\n",
    "results_used = f'{aux_name}/Statistics_{aux_name}_Bootstraps_FullAreas_Data/'\n",
    "\n",
    "processors_used = 7\n",
    "del(aux_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index values of the 5th, 95th and median number, when data are ordered (for the bootstraping)\n",
    "l_min = int(bootstr*5/100)\n",
    "l_max = int(bootstr*95/100)-1\n",
    "l_med = int(bootstr/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = !ls $out_dir$results_used\n",
    "areas_used = sorted(list(set([i.split('_')[0] for i in all_files])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \n",
    "    return (data-data.min('time'))/(data.max('time')-data.min('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dictionary = {i_area: {} for i_area in areas_used+['Med']}\n",
    "for i_area in areas_used:\n",
    "    # get data for reference connections (temporal & Med patterns) to localized extremes\n",
    "    file_prefix = out_dir+results_used+i_area+'_ReferenceConnections_'\n",
    "    file_data = xr.open_dataarray(file_prefix+'ConnTemp.nc')\n",
    "    # in case analysis is for winter and there is no climatoly for 29th Feb, give same values as 28th Feb\n",
    "    if '0229' not in file_data.temporal.values and season_studied!='Summer':\n",
    "        file_data = [file_data, file_data.sel(temporal='0228').assign_coords({'temporal': '0229'})]\n",
    "        file_data = xr.concat(file_data, dim='temporal')\n",
    "        \n",
    "    input_dictionary[i_area]['RefTempCondProb'] = file_data.sel(bootstrap='Actual').drop('bootstrap') \n",
    "    \n",
    "    # get data for Mediterranean patterns (new derivation) and localized extremes\n",
    "    file_prefix = out_dir+results_used+i_area+'_MedPatterns_'\n",
    "    file_data = xr.open_dataset(file_prefix+'CondProbs.nc')['CondProbs']\n",
    "    file_data = file_data.sel(bootstrap='Actual', Constraints=0).drop(['bootstrap', 'Constraints'])\n",
    "    input_dictionary[i_area]['MedCondProb'] = file_data\n",
    "    \n",
    "    # get data for areal precipitation\n",
    "    file_name = f'{out_dir}../Data/Forecasts/{i_area}_precip.nc'\n",
    "    file_data = xr.open_dataarray(file_name)\n",
    "    file_data = file_data.rolling(step=rolling_days).sum().dropna('step')\n",
    "    input_dictionary[i_area]['PrcFrcst'] = file_data\n",
    "    file_name = f'{out_dir}{results_used}{i_area}_PrecipObs_Timeseries.nc'\n",
    "    file_data = xr.open_dataarray(file_name).dropna('time')\n",
    "    if 'Winter' in results_used:\n",
    "        file_data = file_data.isel(time=file_data.time.dt.month.isin([1, 2, 9, 10, 11, 12]))\n",
    "    elif 'Summer' in results_used:\n",
    "        file_data = file_data.isel(time=file_data.time.dt.month.isin([3, 4, 5, 6, 7, 8]))\n",
    "    input_dictionary[i_area]['PrcObs'] = file_data\n",
    "    file_name = f'{out_dir}{results_used}{i_area}_PrecipERA5_CondProbs.nc'\n",
    "    file_data = xr.open_dataarray(file_name)\n",
    "    file_data = file_data.sel(bootstrap='Actual', Constraints=0).drop(['bootstrap', 'Constraints'])\n",
    "    input_dictionary[i_area]['PrcCondProb'] = file_data\n",
    "    \n",
    "    # get data for water vapour flux\n",
    "    file_name = f'{out_dir}{results_used}{i_area}_WvfERA5_CondProbs.nc'\n",
    "    file_data = xr.open_dataarray(file_name)\n",
    "    file_data = file_data.sel(bootstrap='Actual', Constraints=0).drop(['bootstrap', 'Constraints'])\n",
    "    input_dictionary[i_area]['WvfCondProb'] = file_data\n",
    "\n",
    "    file_name = f'{out_dir}../Data/Forecasts/{i_area}_wvf.nc'\n",
    "    file_data = xr.open_dataarray(file_name)\n",
    "    file_data = file_data.rolling(step=rolling_days).mean()\n",
    "    total_area = np.sqrt(file_data.sel(wvf_direction='NorthW')**2+file_data.sel(wvf_direction='EastW')**2)\n",
    "    total_area = total_area.assign_coords({'wvf_direction': 'Total'})\n",
    "    file_data = [file_data.sel(wvf_direction=['NorthW', 'SouthW', 'EastW', 'WestW']), total_area]\n",
    "    file_data = xr.concat(file_data, dim='wvf_direction')\n",
    "    wvf_dircs = file_data.wvf_direction.values\n",
    "    \n",
    "    file_name = f'{out_dir}../Data/Forecasts/{i_area}_relhum.nc'\n",
    "    file_data2 = xr.open_dataarray(file_name).rename({'pressure_level': 'wvf_direction'})\n",
    "    file_data2 = file_data2.assign_coords({'wvf_direction': \n",
    "                                           [f'RH{int(i)}' for i in file_data2.wvf_direction.values]})\n",
    "    file_data2 = file_data2.sel(wvf_direction=input_dictionary[i_area]['WvfCondProb'].Extra.values[1])\n",
    "    file_data2 = file_data2.rolling(step=rolling_days).mean()\n",
    "    file_data = xr.concat([file_data, file_data2], dim='wvf_direction').dropna('step')\n",
    "    input_dictionary[i_area]['WvfFrcst'] = file_data\n",
    "    \n",
    "# get auxiliary data\n",
    "lead_steps_used = []\n",
    "lead_steps_used.append(input_dictionary[i_area]['PrcFrcst'].step.values)\n",
    "lead_steps_used.append(input_dictionary[i_area]['WvfFrcst'].step.values)\n",
    "\n",
    "perc_used = input_dictionary[i_area]['PrcCondProb'].percentile.values.tolist()\n",
    "\n",
    "del(i_area, file_prefix, file_data, file_name, total_area, file_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data for 9 Mediterranean clusters\n",
    "file_data = xr.open_dataarray(frcst_med)\n",
    "file_data = file_data.isel(number=file_data.number>-1) # don't use ens. mean\n",
    "file_data = file_data.sel(rolling=rolling_days, variable_type='Anomalies', ClustersNumber=9)\n",
    "\n",
    "# clim. based on ERA5 and not model-dependent gives better results for up to ~2nd week lead times\n",
    "# file_data = file_data.sel(clim_type='Model')\n",
    "file_data = file_data.sel(clim_type='ERA5')\n",
    "\n",
    "input_dictionary['Med']['PatFrcst'] = file_data.reset_coords(drop=True) \n",
    "\n",
    "lead_steps_used.append(input_dictionary['Med']['PatFrcst'].step.values)\n",
    "\n",
    "del(file_data, frcst_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_steps = set(lead_steps_used[0])\n",
    "for l in lead_steps_used[1:]:\n",
    "    final_steps &= set(l)\n",
    "\n",
    "final_steps = list(final_steps) # convert to list\n",
    "final_steps = sorted(final_steps)\n",
    "del(lead_steps_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (cost_ratio: 22)&gt;\n",
       "array([0.001     , 0.01      , 0.02      , 0.03      , 0.04      ,\n",
       "       0.05      , 0.06      , 0.07      , 0.08      , 0.09      ,\n",
       "       0.1       , 0.18172727, 0.26345455, 0.34518182, 0.42690909,\n",
       "       0.50863636, 0.59036364, 0.67209091, 0.75381818, 0.83554545,\n",
       "       0.91727273, 0.999     ])\n",
       "Coordinates:\n",
       "  * cost_ratio  (cost_ratio) float64 0.001 0.01 0.02 ... 0.8355 0.9173 0.999</pre><div class='xr-wrap' hidden><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>cost_ratio</span>: 22</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-38b439fb-32b3-4799-aef3-e8a1484312ef' class='xr-array-in' type='checkbox' checked><label for='section-38b439fb-32b3-4799-aef3-e8a1484312ef' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.001 0.01 0.02 0.03 0.04 0.05 ... 0.6721 0.7538 0.8355 0.9173 0.999</span></div><div class='xr-array-data'><pre>array([0.001     , 0.01      , 0.02      , 0.03      , 0.04      ,\n",
       "       0.05      , 0.06      , 0.07      , 0.08      , 0.09      ,\n",
       "       0.1       , 0.18172727, 0.26345455, 0.34518182, 0.42690909,\n",
       "       0.50863636, 0.59036364, 0.67209091, 0.75381818, 0.83554545,\n",
       "       0.91727273, 0.999     ])</pre></div></div></li><li class='xr-section-item'><input id='section-58924954-5bdd-43e6-b886-1864df0172f9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-58924954-5bdd-43e6-b886-1864df0172f9' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>cost_ratio</span></div><div class='xr-var-dims'>(cost_ratio)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.001 0.01 0.02 ... 0.9173 0.999</div><input id='attrs-2a38978b-6fbc-4989-b4b8-8a02cce31690' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2a38978b-6fbc-4989-b4b8-8a02cce31690' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-57441900-ca70-43c3-bfcc-cddb775065c2' class='xr-var-data-in' type='checkbox'><label for='data-57441900-ca70-43c3-bfcc-cddb775065c2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0.001   , 0.01    , 0.02    , 0.03    , 0.04    , 0.05    , 0.06    ,\n",
       "       0.07    , 0.08    , 0.09    , 0.1     , 0.181727, 0.263455, 0.345182,\n",
       "       0.426909, 0.508636, 0.590364, 0.672091, 0.753818, 0.835545, 0.917273,\n",
       "       0.999   ])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-143489c2-84bb-4472-93e9-8eece2c40179' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-143489c2-84bb-4472-93e9-8eece2c40179' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray (cost_ratio: 22)>\n",
       "array([0.001     , 0.01      , 0.02      , 0.03      , 0.04      ,\n",
       "       0.05      , 0.06      , 0.07      , 0.08      , 0.09      ,\n",
       "       0.1       , 0.18172727, 0.26345455, 0.34518182, 0.42690909,\n",
       "       0.50863636, 0.59036364, 0.67209091, 0.75381818, 0.83554545,\n",
       "       0.91727273, 0.999     ])\n",
       "Coordinates:\n",
       "  * cost_ratio  (cost_ratio) float64 0.001 0.01 0.02 ... 0.8355 0.9173 0.999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values used for cost loss ratio for the economic value analysis\n",
    "# cost_loss_ratio_values = (100-np.array(perc_used))/100\n",
    "# cost_loss_ratio_values = np.linspace(cost_loss_ratio_values.min()*.8, cost_loss_ratio_values.max()*1.2, 6)\n",
    "cost_loss_ratio_values = np.linspace(0, .1, 11)\n",
    "cost_loss_ratio_values = np.append(cost_loss_ratio_values, np.linspace(cost_loss_ratio_values.max(), 0.999, 12))\n",
    "cost_loss_ratio_values = np.append(cost_loss_ratio_values, (100-np.array(perc_used))/100)\n",
    "cost_loss_ratio_values = np.append(cost_loss_ratio_values, [0.001])\n",
    "cost_loss_ratio_values = sorted(set(cost_loss_ratio_values))[1:]\n",
    "cost_loss_ratio_values = xr.DataArray(cost_loss_ratio_values, dims=['cost_ratio'], \n",
    "                               coords={'cost_ratio': cost_loss_ratio_values})\n",
    "cost_loss_ratio_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_flagging(valid_dates, temp_subset):\n",
    "    \n",
    "    valid_dates = pd.to_datetime(valid_dates)\n",
    "    \n",
    "    if temp_subset == 'All':\n",
    "        temporal_flag = ['All']*len(valid_dates)\n",
    "    elif temp_subset == 'HalfYear':\n",
    "        temporal_flag = (valid_dates.month%12 + 3)//3\n",
    "        temporal_flag = temporal_flag.map({1: 'WinterHalf', 2: 'SummerHalf', 3: 'SummerHalf', 4: 'WinterHalf'})\n",
    "    elif temp_subset == 'Season':\n",
    "        temporal_flag = (valid_dates.month%12 + 3)//3\n",
    "        temporal_flag = temporal_flag.map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'})\n",
    "    elif temp_subset == 'Month':\n",
    "        temporal_flag = valid_dates.month.astype(str)\n",
    "    elif temp_subset == 'DayMonth':\n",
    "        temporal_flag = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag = temporal_flag.values\n",
    "        \n",
    "    return temporal_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autumn', 'Winter']\n"
     ]
    }
   ],
   "source": [
    "# get the climatological frequencies for the dates in each season used for the analysis\n",
    "if 'Winter' in results_used:\n",
    "    seasons_used = ['Autumn', 'Winter']\n",
    "elif 'Summer' in results_used:\n",
    "    seasons_used = ['Spring', 'Summer']\n",
    "else:\n",
    "    seasons_used = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "\n",
    "print(seasons_used)\n",
    "# get the number of days per season for a full 4-year period (so that 1 leap year is also included)\n",
    "aux_climatology = pd.date_range('20000101', '20031231')\n",
    "aux_climatology = pd.DataFrame({'Dates': aux_climatology}, index=aux_climatology)\n",
    "aux_climatology['Season'] = temp_flagging(aux_climatology.Dates.values, 'Season')\n",
    "\n",
    "ref_clim = aux_climatology.query('Season in @seasons_used').Season.value_counts(normalize=True)\n",
    "\n",
    "del(seasons_used, aux_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_extremes_obs(data, percentiles):\n",
    "    \n",
    "    quant_data = data.quantile(np.array(percentiles)/100, interpolation='linear', dim='time') # thresh.\n",
    "    quant_data = quant_data.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "    quant_data = quant_data.assign_coords({'percentile': percentiles}) # assign the dim values based on lags\n",
    "\n",
    "    # boolean xarray for identifying if an event is over the threshold\n",
    "    exceed_xr = [data>quant_data.sel(percentile=i_p) for i_p in percentiles] \n",
    "    exceed_xr = xr.concat(exceed_xr, dim=pd.Index(percentiles, name='percentile')) # concat. data for all perc.\n",
    "   \n",
    "    return exceed_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting subset of forecasted precip/wvf data and generating boolean for exceedance of extremes\n",
    "def define_extremes_frcst(data, percentiles):\n",
    "    \n",
    "    data_stack = data.stack(all_data=['time', 'number'])    \n",
    "    \n",
    "    quants = data_stack.quantile(np.array(percentiles)/100, \n",
    "                                 interpolation='linear', dim='all_data', keep_attrs=True) # thresholds\n",
    "    quants = quants.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "    quants = quants.assign_coords({'percentile': percentiles}) # assign the dim values based on percentiles\n",
    "\n",
    "    # boolean xarray for identifying if an event is over the threshold\n",
    "    exceed_xr = [data>quants.sel(percentile=i_p) for i_p in percentiles] # boolean of exceedance per percentile\n",
    "    exceed_xr = xr.concat(exceed_xr, dim=pd.Index(percentiles, name='percentile')) # concat. data for all perc.\n",
    "    \n",
    "    return exceed_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_subset(area_used, key_used, lead_used):\n",
    "    \n",
    "    frcst_data_all = input_dictionary[area_used][key_used].sel(step=lead_used)\n",
    "    frcst_data_all = frcst_data_all.assign_coords({'time': frcst_data_all.time+lead_used})\n",
    "\n",
    "    return frcst_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brierSS(data_used, ref_score):\n",
    "    \n",
    "    brier_score = data_used\n",
    "    brier_score.name = 'BS'\n",
    "    \n",
    "    brier_ref_clim = ref_score.sel(Method=['DayMonth_Temp', 'Season_Temp']).min('Method')\n",
    "    brier_skill_score = 1 - brier_score/brier_ref_clim\n",
    "    brier_skill_score.name = 'BSS'\n",
    "    \n",
    "    brier_score = xr.merge([brier_score, brier_skill_score])\n",
    "    \n",
    "    return brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def economic_value_calc(input_data):\n",
    "    \n",
    "    frcst_data, obs_data, p_t = input_data\n",
    "    \n",
    "    hits = ((frcst_data>=p_t).where(obs_data==1)).sum('time')\n",
    "    hits.name = 'Hits'    \n",
    "    false_alarms = ((frcst_data>=p_t).where(obs_data==0)).sum('time')\n",
    "    false_alarms.name = 'FalseAlarms'\n",
    "    misses = ((frcst_data<p_t).where(obs_data==1)).sum('time')\n",
    "    misses.name = 'Misses'\n",
    "    cor_neg = ((frcst_data<p_t).where(obs_data==0)).sum('time')\n",
    "    cor_neg.name = 'CorrectNegative'\n",
    "    \n",
    "    hit_rate = hits/(hits+misses)\n",
    "    hit_rate.name = 'HitRate'\n",
    "    false_alarm_rate = false_alarms/(false_alarms+cor_neg)\n",
    "    false_alarm_rate.name = 'FalseAlarmRate'\n",
    "    \n",
    "    ec_val_final = false_alarm_rate*cost_loss_ratio*(1-extremes_occur)\n",
    "    ec_val_final = ec_val_final - hit_rate*extremes_occur*(1-cost_loss_ratio) + extremes_occur\n",
    "    ec_val_final = (ev_clim - ec_val_final)/(ev_clim-extremes_occur*cost_loss_ratio)\n",
    "    ec_val_final = ec_val_final.assign_coords({'p_thr': p_t})\n",
    "    ec_val_final.name = 'EV'\n",
    "    \n",
    "    ec_val_final = xr.merge([ec_val_final, hits, false_alarms, misses, hit_rate, cor_neg, false_alarm_rate])\n",
    "    \n",
    "    return ec_val_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability(input_data):\n",
    "    \n",
    "    frcst_data, obs_data, thresholds = input_data\n",
    "    \n",
    "    thresholds_used = [-.1]+list(thresholds)\n",
    "    \n",
    "    reliab_data = []\n",
    "    for i in range(len(thresholds_used)-1):\n",
    "        used_frcsts = ((frcst_data>thresholds_used[i])&(frcst_data<=thresholds_used[i+1]))\n",
    "        sample_frcsts = used_frcsts.sum('time')\n",
    "        sample_frcsts.name = 'SampleFrcsts'\n",
    "        mean_frcsts = frcst_data.where(used_frcsts).mean('time')\n",
    "        mean_frcsts.name = 'MeanFrcsts'\n",
    "        sample_obs = obs_data.where(used_frcsts).sum('time')\n",
    "        sample_obs.name = 'SampleObs'\n",
    "        mean_obs = obs_data.where(used_frcsts).mean('time')\n",
    "        mean_obs.name = 'MeanObs'\n",
    "        reliab_data.append(xr.merge([sample_frcsts, sample_obs, mean_frcsts, mean_obs]))\n",
    "        \n",
    "    \n",
    "    reliab_data = xr.concat(reliab_data, dim=pd.Index(thresholds_used[1:], name='p_thr'))\n",
    "    \n",
    "    base_rate = obs_data.mean('time')\n",
    "    resolution_final = (reliab_data['MeanObs']-base_rate)**2\n",
    "    resolution_final.name = 'Resolution_Bined'\n",
    "    resolution_total = resolution_final.weighted(reliab_data['SampleFrcsts']).mean('p_thr')\n",
    "    resolution_total.name = 'Resolution_Total'\n",
    "    reliability_final = (reliab_data['MeanFrcsts']-reliab_data['MeanObs'])**2\n",
    "    reliability_final.name = 'Reliability_Bined'\n",
    "    reliability_total = reliability_final.weighted(reliab_data['SampleFrcsts']).mean('p_thr')\n",
    "    reliability_total.name = 'Reliability_Total'\n",
    "    uncertaintly_total = base_rate*(1-base_rate)\n",
    "    uncertaintly_total.name = 'Uncertainty_Total'\n",
    "    \n",
    "    reliab_data = xr.merge([reliab_data, uncertaintly_total, resolution_final, resolution_total,\n",
    "                            reliability_final, reliability_total])\n",
    "    \n",
    "    return reliab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_final(input_data):\n",
    "    \n",
    "    boot_dates_used, calc_ecom_value = input_data\n",
    "    area_i = input_data_analysis['area']\n",
    "    \n",
    "    # define observational extremes\n",
    "    precip_actual_i = input_data_analysis['obs'].sel(time=boot_dates_used)\n",
    "    precip_actual_i = define_extremes_obs(precip_actual_i, perc_used)\n",
    "    \n",
    "    # get reference scores\n",
    "    clim_cond_prob = input_dictionary[area_i]['RefTempCondProb']\n",
    "\n",
    "    seasonal_i = clim_cond_prob.sel(temporal=temp_flagging(boot_dates_used, 'Season'))\n",
    "    seasonal_i = seasonal_i.rename({'temporal': 'time'}).assign_coords({'time': boot_dates_used})\n",
    "    daymonth_i = clim_cond_prob.sel(temporal=temp_flagging(boot_dates_used, 'DayMonth'))\n",
    "    daymonth_i = daymonth_i.rename({'temporal': 'time'}).assign_coords({'time': boot_dates_used})\n",
    "    \n",
    "    ref_forecast_i = [seasonal_i, daymonth_i]\n",
    "    ref_forecast_i = xr.concat(ref_forecast_i, dim=pd.Index(['Season_Temp', 'DayMonth_Temp'], name='Method'))\n",
    "    brier_score_all_ref = ((ref_forecast_i-precip_actual_i)**2).sum('time')/len(boot_dates_used)\n",
    "    \n",
    "    # define forecasted precipitation extremes and get associated cond.prob. and brier score\n",
    "    precip_frcsts_condprobs_i = input_dictionary[area_i]['PrcCondProb']\n",
    "    precip_frcsts_i = input_data_analysis['prc_frc'].sel(time=boot_dates_used)\n",
    "    precip_frcsts_i = define_extremes_frcst(precip_frcsts_i, precip_frcsts_condprobs_i.percentile.values)\n",
    "    precip_frcsts_i = precip_frcsts_i#.rename({'percentile': 'ERA5_percentile'})\n",
    "    \n",
    "    condprob_pos_precip = precip_frcsts_i.where(precip_frcsts_i==1)\n",
    "    condprob_pos_precip = condprob_pos_precip*precip_frcsts_condprobs_i.sel(extr_predictor=1)\n",
    "    condprob_neg_precip = (precip_frcsts_i.where(precip_frcsts_i==0)+1)\n",
    "    condprob_neg_precip = condprob_neg_precip*precip_frcsts_condprobs_i.sel(extr_predictor=0)\n",
    "\n",
    "    precip_frcsts_final_i = (condprob_pos_precip.fillna(0)+condprob_neg_precip.fillna(0)).mean('number')\n",
    "    brier_precip_frcsts_i = ((precip_frcsts_final_i-precip_actual_i)**2).sum('time')/len(boot_dates_used)\n",
    "    \n",
    "    # get brier score when only considering boolean based on actual forecasts and not conditioned on ERA5\n",
    "    precip_fr_actual_i = precip_frcsts_i.mean('number')\n",
    "    precip_fr_actual_i = precip_fr_actual_i.sel(percentile=precip_actual_i.percentile.values) # only obs. perc.\n",
    "    brier_precip_frcsts_actual_i = ((precip_fr_actual_i-precip_actual_i)**2).sum('time')/len(boot_dates_used) \n",
    "     \n",
    "    # define forecasted water vapour flux extremes and get associated cond.prob. and brier score\n",
    "    wvf_frcsts_condprobs_i = input_dictionary[area_i]['WvfCondProb']\n",
    "    wvf_frcsts_i = input_data_analysis['wvf_frc'].sel(time=boot_dates_used)\n",
    "    rh_level = wvf_frcsts_condprobs_i.Extra.values[1]\n",
    "    \n",
    "    rh_alone = wvf_frcsts_i.sel(wvf_direction=rh_level)\n",
    "    norm = normalize(wvf_frcsts_i)    \n",
    "    norm1 = (norm+normalize(rh_alone).drop('wvf_direction').reset_coords(drop=True))/2\n",
    "    norm1 = norm1.expand_dims({'Extra': [rh_level]})\n",
    "    wvf_frcsts_i = wvf_frcsts_i.expand_dims({'Extra': ['Alone']})\n",
    "    wvf_frcsts_i = xr.concat([wvf_frcsts_i, norm1], dim='Extra')     \n",
    "    wvf_frcsts_i_actual = wvf_frcsts_i\n",
    "    wvf_frcsts_i = define_extremes_frcst(wvf_frcsts_i, wvf_frcsts_condprobs_i.percentile.values)\n",
    "    \n",
    "    condprob_pos_wvf = wvf_frcsts_i.where(wvf_frcsts_i==1)\n",
    "    condprob_pos_wvf = condprob_pos_wvf*wvf_frcsts_condprobs_i.sel(extr_predictor=1)\n",
    "    condprob_neg_wvf = (wvf_frcsts_i.where(wvf_frcsts_i==0)+1)\n",
    "    condprob_neg_wvf = condprob_neg_wvf*wvf_frcsts_condprobs_i.sel(extr_predictor=0)\n",
    "\n",
    "    wvf_frcsts_final_i = (condprob_pos_wvf.fillna(0)+condprob_neg_wvf.fillna(0)).mean('number')\n",
    "    brier_wvf_frcsts_i = ((wvf_frcsts_final_i-precip_actual_i)**2).sum('time')/len(boot_dates_used)   \n",
    "   \n",
    "    # create wvf_percentile = 0 for the next combined predictors of patterns and wvf\n",
    "    wvf_frcsts_i = define_extremes_frcst(wvf_frcsts_i_actual, [80]).rename({'percentile': 'wvf_percentile'})\n",
    "    wvf_frcsts_i = wvf_frcsts_i.assign_coords({'wvf_percentile': [1]})\n",
    "    wvf_frcsts_i_p0 = (wvf_frcsts_i.isel(wvf_percentile=0)>=0).assign_coords(wvf_percentile=0)\n",
    "    wvf_frcsts_i = xr.concat([wvf_frcsts_i_p0, wvf_frcsts_i], dim='wvf_percentile')\n",
    "\n",
    "    # define forecasted mediterranean patterns and get associated cond.prob. and brier score\n",
    "    medpat_frcsts_condprobs_i = input_dictionary[area_i]['MedCondProb']\n",
    "    medpat_frcsts_i = input_data_analysis['med_frc'].sel(time=boot_dates_used)\n",
    "    \n",
    "    # make labels boolean for each day and cluster\n",
    "    alloc_i = [medpat_frcsts_i==i for i in range(len(medpat_frcsts_condprobs_i.cluster))]\n",
    "    alloc_i = xr.concat(alloc_i, dim=medpat_frcsts_condprobs_i.cluster)\n",
    "\n",
    "    brier_Med_wvf = []\n",
    "    frcst_Med_wvf = []\n",
    "    for i_warnarea in medpat_frcsts_condprobs_i.WarnArea:\n",
    "        allocs_i_warn_pos = alloc_i.where(wvf_frcsts_i.sel(WarnArea=i_warnarea)).fillna(0).astype(int)\n",
    "        allocs_i_warn_neg = alloc_i.where(wvf_frcsts_i.sel(WarnArea=i_warnarea)==0).fillna(0).astype(int)\n",
    "\n",
    "        allocs_i_warnarea = xr.concat([allocs_i_warn_pos, allocs_i_warn_neg], \n",
    "                                      dim=medpat_frcsts_condprobs_i.CondProbType)\n",
    "\n",
    "        frcst_Med_i_warn = allocs_i_warnarea*medpat_frcsts_condprobs_i.sel(WarnArea=i_warnarea)\n",
    "        frcst_Med_i_warn = frcst_Med_i_warn.sum(['cluster', 'CondProbType']).mean('number')\n",
    "\n",
    "        brier_Med_i_warn = (frcst_Med_i_warn-precip_actual_i.sel(WarnArea=i_warnarea))**2\n",
    "        brier_Med_i_warn = brier_Med_i_warn.sum('time')/len(boot_dates_used)\n",
    "        brier_Med_wvf.append(brier_Med_i_warn)\n",
    "        frcst_Med_wvf.append(frcst_Med_i_warn)\n",
    "\n",
    "    brier_Med_wvf = xr.concat(brier_Med_wvf, dim='WarnArea')\n",
    "    frcst_Med_wvf = xr.concat(frcst_Med_wvf, dim='WarnArea')\n",
    "    \n",
    "    # calculate BSS\n",
    "    step_tag = brier_precip_frcsts_i.step.values\n",
    "    brier_score_all_ref_final = brierSS(brier_score_all_ref, brier_score_all_ref)\n",
    "    brier_score_all_ref_final = brier_score_all_ref_final.assign_coords({'step': step_tag})\n",
    "    brier_precip_frcsts_actual_i = brierSS(brier_precip_frcsts_actual_i, brier_score_all_ref)\n",
    "    brier_precip_frcsts_i = brierSS(brier_precip_frcsts_i, brier_score_all_ref)\n",
    "    brier_wvf_frcsts_i = brierSS(brier_wvf_frcsts_i, brier_score_all_ref)\n",
    "    brier_Med_wvf = brierSS(brier_Med_wvf, brier_score_all_ref)\n",
    "    \n",
    "    # calculate economic value (if needed)\n",
    "    if calc_ecom_value:\n",
    "        # generate auxiliary data needed for calculating the Economic Value for different Cost/Loss measures\n",
    "        global extremes_occur, cost_loss_ratio, ev_clim\n",
    "        extremes_occur = precip_actual_i.mean('time')\n",
    "        cost_loss_ratio = extremes_occur*0+cost_loss_ratio_values # generate remaining dims of the cost ratio\n",
    "        extremes_occur = cost_loss_ratio*0+extremes_occur # generate \"cost_loss_ratio\" dim on \"extremes_occur\"\n",
    "        ev_clim = xr.concat([cost_loss_ratio, extremes_occur], dim='clim').min('clim') # clim gain for each area\n",
    "        \n",
    "        threshold_bins = 25 # was 15 before\n",
    "        \n",
    "        # EV of reference forecasts\n",
    "        thresholds_used_max = input_dictionary[area_i]['RefTempCondProb'].max().values\n",
    "        thresholds_used = list(np.linspace(0, np.floor(thresholds_used_max*100)/100, threshold_bins))\n",
    "        ec_val_ref = list(product([ref_forecast_i], [precip_actual_i], thresholds_used))\n",
    "        ec_val_ref = [economic_value_calc(i) for i in ec_val_ref]\n",
    "        ec_val_ref = xr.concat(ec_val_ref, dim='p_thr')\n",
    "        ec_val_ref = ec_val_ref.assign_coords({'step': step_tag})\n",
    "        rel_data = reliability([ref_forecast_i, precip_actual_i, thresholds_used])\n",
    "        ec_val_ref = xr.merge([ec_val_ref, rel_data])\n",
    "        \n",
    "        # EV of direct forecasts (using forecasted precip and cond. probs based on ERA5)\n",
    "        thresholds_used_max = input_dictionary[area_i]['PrcCondProb'].max().values\n",
    "        thresholds_used = list(np.linspace(0, np.floor(thresholds_used_max*100)/100, threshold_bins))\n",
    "        ec_val_precip = list(product([precip_frcsts_final_i], [precip_actual_i], thresholds_used))\n",
    "        ec_val_precip = [economic_value_calc(i) for i in ec_val_precip]\n",
    "        ec_val_precip = xr.concat(ec_val_precip, dim='p_thr')\n",
    "        rel_data = reliability([precip_frcsts_final_i, precip_actual_i, thresholds_used])\n",
    "        ec_val_precip = xr.merge([ec_val_precip, rel_data])\n",
    "        \n",
    "        # EV of direct forecasts (using forecasted precip and boolean based only on forecasts)\n",
    "        thresholds_used = list(np.linspace(0, 1, len(precip_frcsts_i.number)+1))\n",
    "        ec_val_precip_act = list(product([precip_fr_actual_i], [precip_actual_i], thresholds_used))\n",
    "        ec_val_precip_act = [economic_value_calc(i) for i in ec_val_precip_act]\n",
    "        ec_val_precip_act = xr.concat(ec_val_precip_act, dim='p_thr')\n",
    "        rel_data = reliability([precip_fr_actual_i, precip_actual_i, thresholds_used])\n",
    "        ec_val_precip_act = xr.merge([ec_val_precip_act, rel_data])\n",
    "        \n",
    "        # EV of forecasts using wvf\n",
    "        thresholds_used_max = input_dictionary[area_i]['WvfCondProb'].max().values\n",
    "        thresholds_used = list(np.linspace(0, np.floor(thresholds_used_max*100)/100, threshold_bins))\n",
    "        ec_val_wvf = list(product([wvf_frcsts_final_i], [precip_actual_i], thresholds_used))\n",
    "        ec_val_wvf = [economic_value_calc(i) for i in ec_val_wvf]\n",
    "        ec_val_wvf = xr.concat(ec_val_wvf, dim='p_thr')\n",
    "        rel_data = reliability([wvf_frcsts_final_i, precip_actual_i, thresholds_used])\n",
    "        ec_val_wvf = xr.merge([ec_val_wvf, rel_data])\n",
    "        \n",
    "        # EV of forecasts using Mediterranean patterns\n",
    "        thresholds_used_max = input_dictionary[area_i]['MedCondProb'].sel(wvf_percentile=0).max().values\n",
    "        thresholds_used = list(np.linspace(0, np.floor(thresholds_used_max*100)/100, threshold_bins))\n",
    "        ec_val_Medpat = list(product([frcst_Med_wvf], [precip_actual_i], thresholds_used))\n",
    "        ec_val_Medpat = [economic_value_calc(i) for i in ec_val_Medpat]\n",
    "        ec_val_Medpat = xr.concat(ec_val_Medpat, dim='p_thr')\n",
    "        rel_data = reliability([frcst_Med_wvf, precip_actual_i, thresholds_used])\n",
    "        ec_val_Medpat = xr.merge([ec_val_Medpat, rel_data])\n",
    "    \n",
    "    if calc_ecom_value==False:\n",
    "        return {'ReferenceConnections': {'Brier': brier_score_all_ref_final.astype('float32')}, \n",
    "                'precipERA5': {'Brier': brier_precip_frcsts_i.astype('float32')},\n",
    "                'precipForecasts': {'Brier': brier_precip_frcsts_actual_i.astype('float32')},\n",
    "                'wvfERA5': {'Brier': brier_wvf_frcsts_i.astype('float32')}, \n",
    "                'MedPatterns': {'Brier': brier_Med_wvf.astype('float32')}}\n",
    "    else:\n",
    "        return {'ReferenceConnections': {'Brier': brier_score_all_ref_final.astype('float32'), \n",
    "                                         'EV': ec_val_ref.astype('float32')}, \n",
    "                'precipERA5': {'Brier': brier_precip_frcsts_i.astype('float32'), \n",
    "                               'EV': ec_val_precip.astype('float32')},\n",
    "                'precipForecasts': {'Brier': brier_precip_frcsts_actual_i.astype('float32'), \n",
    "                                    'EV': ec_val_precip_act.astype('float32')},\n",
    "                'wvfERA5': {'Brier': brier_wvf_frcsts_i.astype('float32'), \n",
    "                            'EV': ec_val_wvf.astype('float32')}, \n",
    "                'MedPatterns': {'Brier': brier_Med_wvf.astype('float32'), \n",
    "                                'EV': ec_val_Medpat.astype('float32')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_bootstraps(area_used, step_used):\n",
    "    \n",
    "    precip_frcsts = input_dictionary[area_used]['PrcFrcst'].sel(step=step_used)\n",
    "    precip_frcsts = precip_frcsts.assign_coords({'time': precip_frcsts.time+step_used})\n",
    "\n",
    "    precip_actual = input_dictionary[area_used]['PrcObs']\n",
    "     \n",
    "    global common_dates\n",
    "    common_dates = set(precip_frcsts.time.values) & set(precip_actual.time.values) # get common dates\n",
    "    common_dates = sorted(common_dates) # convert to sorted list\n",
    "    common_dates = pd.to_datetime(common_dates)\n",
    "\n",
    "    precip_actual = precip_actual.sel(time=common_dates)\n",
    "\n",
    "    wvf_frcsts = frcst_subset(area_used, 'WvfFrcst', step_used).sel(time=common_dates)\n",
    "    medpat_frcsts = frcst_subset('Med', 'PatFrcst', step_used).sel(time=common_dates)\n",
    "    precip_frcsts = precip_frcsts.sel(time=common_dates)\n",
    "    \n",
    "    global input_data_analysis\n",
    "    input_data_analysis = {'obs': precip_actual, 'prc_frc': precip_frcsts, 'wvf_frc': wvf_frcsts,\n",
    "                           'med_frc': medpat_frcsts, 'area': area_used}\n",
    "\n",
    "    dates_df = pd.DataFrame({'Dates': common_dates}, index=common_dates)\n",
    "    dates_df['Season'] = temp_flagging(common_dates, 'Season')\n",
    "    \n",
    "    # get bootstraps and actual dates of analysis that respect the climatological frequencies of seasons\n",
    "    initial_freq = dates_df.Season.value_counts(normalize=True).values\n",
    "    initial_inst = dates_df.Season.value_counts(normalize=False)\n",
    "    ref_score = ref_clim.loc[initial_inst.index].values\n",
    "    ratio_seasons = ref_score/ref_score[-1] # last value has lowest instances (value_counts sorted as default)\n",
    "    seasons_len_final = (ratio_seasons*initial_inst.values[-1]).astype(int)\n",
    "\n",
    "    check_len = initial_inst.values < seasons_len_final\n",
    "    if check_len.sum()>0:\n",
    "        min_loc = np.argmin(seasons_len_final[check_len])\n",
    "        min_loc = (np.where(check_len==True)[0])[min_loc]\n",
    "        ratio_seasons = ref_score/ref_score[min_loc]\n",
    "        seasons_len_final = (ratio_seasons*initial_inst.values[min_loc]).astype(int)\n",
    "\n",
    "    seasons_len_final = pd.Series(seasons_len_final, index=initial_inst.index)\n",
    "\n",
    "    bbs_dates = []\n",
    "    for i_index, i_len in enumerate(seasons_len_final):\n",
    "        i_season = seasons_len_final.index[i_index]\n",
    "        all_dates_season = dates_df.query('Season==@i_season').index\n",
    "        np.random.seed(10)\n",
    "        bbs_i_ssn = np.random.choice(all_dates_season, i_len*bootstr) # generate all bootstrapped values\n",
    "        bbs_i_ssn = pd.to_datetime(bbs_i_ssn) # convert to datetime\n",
    "        bbs_i_ssn = np.array_split(bbs_i_ssn, bootstr) # split into the number of subsets (samples)\n",
    "        i_actual = pd.to_datetime(np.array(all_dates_season[:i_len]))\n",
    "        bbs_i_ssn = np.insert(bbs_i_ssn, 0, i_actual, axis=0) # add actual at 1st place\n",
    "        bbs_dates.append(bbs_i_ssn)\n",
    "\n",
    "    bbs_dates = np.concatenate(bbs_dates, axis=1)\n",
    "    bbs_dates = [(j, True) if i==0 else (j, True) for i, j in enumerate(bbs_dates)]\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=processors_used) # object for multiprocessing\n",
    "    res_bbs = list(tqdm.tqdm(pool.imap(statistical_analysis_final, bbs_dates), \n",
    "                             total=len(bbs_dates), position=0, leave=True))\n",
    "    pool.close()\n",
    "    del(pool)\n",
    "    \n",
    "    return res_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_statistics(key_used, indicator_used):\n",
    "    \n",
    "    data_final_all = [] # loop though warning zones, otherwise data get out of memory\n",
    "    for i_wa in input_dictionary[i_area]['PrcObs'].WarnArea.values:\n",
    "        \n",
    "        bs_all_bootstraps = xr.concat([i[key_used][indicator_used].sel(WarnArea=i_wa) for i in res_bbs],\n",
    "                                               dim=pd.Index(range(bootstr+1), name='bootstrap'))\n",
    "\n",
    "        # process bootstraps for getting the results from the Q5, Q95, Median, and Actual bootstraps\n",
    "        data_final = bs_all_bootstraps.to_array().rename({'variable': 'Indicator'})\n",
    "        data_final = data_final.transpose(..., 'bootstrap')\n",
    "\n",
    "        # get percent of bootstraps that BSS is positive (for checking significance of results)\n",
    "        if indicator_used=='Brier':\n",
    "            sign = (data_final.sel(Indicator='BSS')>0).sum('bootstrap')/(bootstr+1)\n",
    "            sign = sign.expand_dims('Indicator').to_dataset('Indicator')\n",
    "            sign = sign.rename({'BSS': 'BSS_Sign'})\n",
    "\n",
    "        # get the quantiles of interest based on bootstraps\n",
    "        data_quant = np.sort(data_final.isel(bootstrap=data_final.bootstrap>0), axis=-1)[..., [l_min, l_max]]\n",
    "        data_quant = data_final.sel(bootstrap=range(2))*0+data_quant\n",
    "        data_quant = data_quant.assign_coords({'bootstrap': ['Q5', 'Q95']})\n",
    "\n",
    "        # add results from original analysis\n",
    "        data_actual = data_final.isel(bootstrap=0).assign_coords({'bootstrap': 'Actual'})\n",
    "\n",
    "        # get median value based on bootstraps and actual data (so median is actual sorted index)\n",
    "        data_median = np.sort(data_final, axis=-1)[..., l_med]\n",
    "        data_median = data_final.sel(bootstrap=0)*0+data_median\n",
    "        data_median = data_median.assign_coords({'bootstrap': 'Q50'})\n",
    "\n",
    "        data_final = xr.concat([data_quant, data_actual, data_median], dim='bootstrap')\n",
    "        if indicator_used=='Brier':\n",
    "            data_final = data_final.to_dataset('Indicator')\n",
    "            data_final = xr.merge([data_final, sign])\n",
    "        else:\n",
    "            data_final = data_final.to_dataset('Indicator')#.max('p_thr')\n",
    "            data_final['Hits'] = data_final['Hits'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['FalseAlarms'] = data_final['FalseAlarms'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['HitRate'] = data_final['HitRate'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['FalseAlarmRate'] = data_final['FalseAlarmRate'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['SampleFrcsts'] = data_final['SampleFrcsts'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['MeanFrcsts'] = data_final['MeanFrcsts'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['SampleObs'] = data_final['SampleObs'].isel(cost_ratio=0, drop=True)\n",
    "            data_final['MeanObs'] = data_final['MeanObs'].isel(cost_ratio=0, drop=True)\n",
    "        \n",
    "        data_final_all.append(data_final)\n",
    "    \n",
    "    data_final_all = xr.concat(data_final_all, dim='WarnArea')\n",
    "    \n",
    "    return data_final_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [36:56<00:00,  2.21s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:14<00:00,  2.23s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:15<00:00,  2.23s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [36:44<00:00,  2.20s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:47<00:00,  2.27s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:42<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:26<00:00,  2.24s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:11<00:00,  2.23s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:42<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:47<00:00,  2.27s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:37<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:44<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:40<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [38:12<00:00,  2.29s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:06<00:00,  2.22s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:43<00:00,  2.26s/it]\n",
      "100%|███████████████████████████████████████| 1001/1001 [37:40<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "tps_an = ['ReferenceConnections', 'precipERA5', 'precipForecasts', 'wvfERA5', 'MedPatterns']\n",
    "\n",
    "results_all = {i_area: {i_type: {} for i_type in tps_an} for i_area in areas_used}\n",
    "for i_area in areas_used[:]:\n",
    "    results_all[i_area] = {i_type: {'Brier': [], 'EV': []} for i_type in tps_an}\n",
    "    for i_step in np.array(final_steps)[:17]:#[:8:3]:#[[2, 4, 7]][:1]:\n",
    "        res_bbs = analysis_bootstraps(i_area, i_step)\n",
    "        for i_type in tps_an:\n",
    "            results_all[i_area][i_type]['Brier'].append( bootstrap_statistics(i_type, 'Brier') )     \n",
    "            results_all[i_area][i_type]['EV'].append( bootstrap_statistics(i_type, 'EV') )\n",
    "        del(res_bbs)\n",
    "        \n",
    "    for i_type in tps_an:\n",
    "        results_all[i_area][i_type]['Brier'] = xr.concat(results_all[i_area][i_type]['Brier'], dim='step')\n",
    "        file_name = f'{out_dir}{results_used}{i_area}_EPEsForecst_{i_type}_Brier.nc'\n",
    "        results_all[i_area][i_type]['Brier'].to_netcdf(file_name)\n",
    "        results_all[i_area][i_type]['EV'] = xr.concat(results_all[i_area][i_type]['EV'], dim='step')\n",
    "        file_name = f'{out_dir}{results_used}{i_area}_EPEsForecst_{i_type}_EV.nc'\n",
    "        results_all[i_area][i_type]['EV'].to_netcdf(file_name)\n",
    "        \n",
    "del(i_area, i_step, i_type, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
