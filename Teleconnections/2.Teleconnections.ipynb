{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.api import OLS, add_constant\n",
    "from sklearn.feature_selection import f_regression#, mutual_info_regression\n",
    "\n",
    "from itertools import product, combinations, groupby\n",
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr#, GPDC, CMIknn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = ''\n",
    "output_loc = dir_loc+'Deliverable/'\n",
    "!mkdir $output_loc\n",
    "temp_resolution = 'd'\n",
    "temp_aggr = 5\n",
    "months_analysed = [9, 10, 11, 12, 1, 2]\n",
    "max_shift = 5 # for Correlation: negative values refer to pattern's forecasting, positive to forecasting indices\n",
    "t_min, t_max = 1, 5 # for Tigramite: positive values refers to forecating of patterns\n",
    "ols_min, ols_max = -2, 6 # for OLS: positive values relate to forecasting of patterns\n",
    "pat_names_short = ['AtlL', 'BscL', 'IbrL', 'SclL', 'BlkL', 'BlSL', 'MedH', 'MnrL', 'MnrH'] # short names \n",
    "pat_colors = [(.02, 0.40, 0.55), (.0, 0.75, 0.7), (.0, 0.63, 0.4), (.85, 0.35, 0.1), (.8, 0.5, 0.75), \n",
    "              (.7, 0.5, 0.45), (1, 0.7, 0.75), (.65, 0.65, 0.65), (.95, 0.95, 0.2)]\n",
    "color_palette = {i: j for i, j in zip(pat_names_short, pat_colors)}    \n",
    "\n",
    "original_plotting = sns.plotting_context() # original plotting content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "ts_patterns = xr.open_dataarray(dir_loc+'MedIndices.nc')\n",
    "ts_indices = xr.open_dataarray(dir_loc+'AtmIndices.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ts_patterns.cluster.values\n",
    "new_names = [i if i<0 else pat_names_short[i] for i in old_names]\n",
    "ts_patterns = ts_patterns.assign_coords({'cluster': new_names})\n",
    "del(old_names, new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Main indices used for the analysis\"\n",
    "indices_used_feat_sel = ['NAO', 'AO', 'MOI1', 'WeMO', 'PrecipSahel', 'ArcticEurope']\n",
    "indices_used_feat_sel += [f'SST_{i}' for i in range(1, 7)]+[f'SML1_{i}' for i in range(1, 7)]\n",
    "indices_used_feat_sel += ['MJO_RMM1', 'MJO_RMM2']\n",
    "\n",
    "# new names for the plots\n",
    "indices_used_final_names = {i:i for i in indices_used_feat_sel}\n",
    "for i in indices_used_final_names:\n",
    "    if i=='MOI1': indices_used_final_names[i]='MO'\n",
    "    if 'SML1' in i: indices_used_final_names[i]='SML'+indices_used_final_names[i][-2:]\n",
    "     \n",
    "\"Main combinations used for the analysis\"\n",
    "main_type_used = 'InterAnnRemov'\n",
    "main_indicator_used = 'projection_norm'\n",
    "\n",
    "del(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_patterns and atm_var_data (later on) have same dates, as ts_patterns is derived from that data\n",
    "common_dates = set(ts_patterns.time.values) & set(ts_indices.time.values)\n",
    "common_dates = sorted(common_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep common dates and resample to the temporal resolution of interest\n",
    "ts_patterns_final = ts_patterns.sel(time=sorted(common_dates))\n",
    "ts_patterns_final = ts_patterns_final.resample(time=f'{temp_aggr}{temp_resolution}').mean('time')\n",
    "\n",
    "ts_indices_final = ts_indices.sel(time=sorted(common_dates))\n",
    "ts_indices_final = ts_indices_final.resample(time=f'{temp_aggr}{temp_resolution}').mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1)\n",
    "cross_cor = ts_patterns_final.sel(Type=main_type_used, Indicator=main_indicator_used, cluster=pat_names_short)\n",
    "cross_cor = cross_cor.isel(time=cross_cor.time.dt.month.isin(months_analysed))\n",
    "cross_cor = cross_cor.to_dataframe('S').pivot_table(index='time', columns='cluster', values='S')\n",
    "cross_cor = cross_cor[pat_names_short] # keep the actual order (before it goes alphabetically)\n",
    "cross_cor = cross_cor.corr()\n",
    "mask_used = np.triu(cross_cor)\n",
    "ax = sns.heatmap(cross_cor, cmap='RdBu_r', vmin=-1, vmax=1, annot=True, fmt='.2f', mask=mask_used)\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')\n",
    "plt.savefig(f'{output_loc}CrossCorr{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "del(cross_cor, mask_used, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_correlation(input_data):\n",
    "    \n",
    "    used_data, i_shift = input_data\n",
    "    \n",
    "    # shift works correctly, cause the data are on continuous time, without gaps (besides common aggregation)\n",
    "    used_data_shifted = used_data.shift(time=i_shift).dropna('time') # drop NaN, otherwise dask has memory issues\n",
    "    used_data_shifted = used_data_shifted.isel(time=used_data_shifted.time.dt.month.isin(months_analysed))\n",
    "    \n",
    "    ts_patterns_final_used = ts_patterns_final.sel(time=used_data_shifted.time.values)\n",
    "    \n",
    "    lagged_cor = xr.corr(ts_patterns_final_used, used_data_shifted, dim='time')\n",
    "    lagged_cor.name = 'correlation'\n",
    "    lagged_cor_final = xr.merge([lagged_cor])\n",
    "\n",
    "    # because shift moves values at the next timestep, the lag is reversed. E.g. for shift 1, we have a\n",
    "    # forecasting information of 1 timestep earlier, thus lag should be -1.\n",
    "    lagged_cor_final = lagged_cor_final.assign_coords({'lag': -i_shift})\n",
    "    \n",
    "    return lagged_cor_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lags = np.arange(-max_shift, max_shift+1).tolist()\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "combs = list(product([ts_indices_final], all_lags))\n",
    "correlations_indices = list(tqdm.tqdm(pool.imap(lagged_correlation, combs), total=len(combs), position=0))\n",
    "pool.close(); pool.join()\n",
    "correlations_indices = xr.concat(correlations_indices, dim='lag').sortby('lag')\n",
    "correlations_indices.to_netcdf(f'{output_loc}CorrelationsIndices_{temp_aggr}{temp_resolution}.nc')\n",
    "\n",
    "del(combs, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_ind = correlations_indices['correlation'].sel(index=indices_used_feat_sel)\n",
    "plot_ind = plot_ind.assign_coords({'index': [indices_used_final_names[i] for i in plot_ind.index.values]})\n",
    "plot_ind = plot_ind.sel(cluster=pat_names_short).sel(Type='InterAnnRemov', Indicator='projection_norm')\n",
    "plot_ind = plot_ind.to_dataframe('Corr').reset_index()\n",
    "plot_ind.rename(columns={'cluster': 'Med. Pat.'}, inplace=True)\n",
    "\n",
    "sns.set_context('paper', font_scale=2)\n",
    "plot_ind = sns.relplot(data=plot_ind, hue='Med. Pat.', x='lag', y='Corr', kind='line',\n",
    "                       col='index', col_wrap=4, linewidth=3, palette=color_palette)\n",
    "for i_ax in plot_ind.axes.flatten():\n",
    "    i_ax.axvline(0, color='grey', linestyle='--')\n",
    "    i_ax.axhline(0, color='grey', linestyle='--')\n",
    "    i_ax.set_xticks(np.arange(-max_shift, max_shift+1))\n",
    "    \n",
    "plot_ind.fig.savefig(f'{output_loc}CorrelationsIndicesPlot_{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "\n",
    "del(plot_ind, i_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal connections (tigramite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_pat_df = ts_patterns_final.sel(Type=main_type_used, Indicator=main_indicator_used).to_dataframe('Value')\n",
    "ts_pat_df = ts_pat_df.pivot_table(index='time', columns='cluster', values='Value')\n",
    "\n",
    "ts_ind_df = ts_indices_final.sel(Type=main_type_used).sel(index=indices_used_feat_sel).to_dataframe('type_used')\n",
    "ts_ind_df = ts_ind_df.pivot_table(index='time', columns='index', values='type_used')\n",
    "ts_ind_df.columns = [indices_used_final_names[i] for i in ts_ind_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causality_tigramite(pat_used):\n",
    "    \n",
    "    ts_final = pd.concat([ts_pat_df[pat_used], ts_ind_df], axis=1) # get final dataset with all indices\n",
    "     \n",
    "    data_mask = ~(ts_final.index.month.isin(months_analysed))*1 # keep only selected months for causality\n",
    "    data_mask = np.repeat(data_mask[:, np.newaxis], len(ts_final.columns), axis=1)\n",
    "    \n",
    "    dataframe = pp.DataFrame(ts_final.values, var_names=ts_final.columns, mask=data_mask) # tigramite object\n",
    "    \n",
    "    \n",
    "    parcorr = ParCorr(mask_type='y') # mask y\n",
    "#     gpdc = GPDC(significance='analytic', gp_params=None, mask_type='y')\n",
    "#     cmikn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', mask_type='y')\n",
    "\n",
    "    used_test = parcorr\n",
    "    \n",
    "    # make the analysis only for the parents of the selected pattern, and not for all connections\n",
    "    selected_links = {}\n",
    "    for j in range(data_mask.shape[1]):\n",
    "        selected_links[j] = [(var, -lag) for var in range(data_mask.shape[1]) \n",
    "                             for lag in range(t_min, t_max + 1)]\n",
    "#         if j ==0 :\n",
    "#             selected_links[j] = [(var, -lag) for var in range(data_mask.shape[1]) \n",
    "#                                  for lag in range(t_min, t_max + 1)]\n",
    "#         else:\n",
    "#             selected_links[j] = [] \n",
    "            \n",
    "    pcmci_cond_test = PCMCI(dataframe=dataframe, cond_ind_test=used_test, verbosity=0) # create the final object\n",
    "    \n",
    "    results = pcmci_cond_test.run_pcmci(tau_min=t_min, tau_max=t_max, pc_alpha=None, # get the causal relations\n",
    "                                        alpha_level = 0.01, selected_links=selected_links)\n",
    "    \n",
    "    results['var_names'] = ts_final.columns\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pat = ts_patterns.cluster.values \n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "caus_tigr = list(tqdm.tqdm(pool.imap(causality_tigramite, all_pat), total=len(all_pat), position=0))\n",
    "pool.close(); pool.join()\n",
    "caus_tigr = {i:j for i, j in zip(all_pat, caus_tigr)}\n",
    "\n",
    "del(all_pat, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max value of \"autocorrelations\" and \"crosscorrelations\"\n",
    "caus_tigr_non_diag = {i: j['val_matrix'].copy() for i, j in caus_tigr.items()}\n",
    "for i_pat in caus_tigr_non_diag:\n",
    "    for i in range(caus_tigr_non_diag[i_pat].shape[2]):\n",
    "        np.fill_diagonal(caus_tigr_non_diag[i_pat][:,:,i], 0)\n",
    "        \n",
    "max_cross = [np.abs(caus_tigr_non_diag[i]).max() for i in pat_names_short]\n",
    "max_cross = np.ceil(max(max_cross)*20)/20\n",
    "max_auto = [np.max(np.abs(np.diagonal(caus_tigr[i]['val_matrix']))) for i in pat_names_short]\n",
    "max_auto = np.ceil(max(max_auto)*20)/20\n",
    "max_auto = max(0.5, max_auto)\n",
    "\n",
    "del(i_pat, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=2)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "c_map_ax1 = fig.add_axes([0.15, 0.05, 0.3, 0.02])\n",
    "ticks_auto = np.linspace(0, max_auto, 5)\n",
    "cbar1 = ColorbarBase(c_map_ax1, orientation='horizontal', cmap='OrRd', ticks=ticks_auto, \n",
    "                     norm=Normalize(vmin=0, vmax=max_auto), label='auto-MCI')\n",
    "c_map_ax2 = fig.add_axes([0.55, 0.05, 0.3, 0.02])\n",
    "ticks_cross = [-max_cross*2/2.5, -max_cross/2.5, 0, max_cross/2.5, max_cross*2/2.5]\n",
    "cbar2 = ColorbarBase(c_map_ax2, orientation='horizontal', cmap='RdBu_r', ticks=ticks_cross, \n",
    "                     norm=Normalize(vmin=-max_cross, vmax=max_cross), label='cross-MCI')\n",
    "\n",
    "for i_c, i in enumerate(pat_names_short):\n",
    "    \n",
    "    # keep only the climatic indices that are causaly connected with the Mediterranean pattern of interest\n",
    "    kep_ind = (caus_tigr[i]['graph']=='')[:, 0, :].sum(axis=(1))\n",
    "    kep_ind = kep_ind != (t_max-t_min+2)\n",
    "    \n",
    "    # remove all cross links that are not from the indicators to the pattern\n",
    "    matrix_used = caus_tigr[i]['val_matrix'].copy()\n",
    "    graph_used = caus_tigr[i]['graph'].copy()\n",
    "    for j in range(1, graph_used.shape[0]):\n",
    "        graph_used[np.delete(np.arange(graph_used.shape[0]), j), j, :] = ''\n",
    "    \n",
    "    caus_plot = tp.plot_graph(\n",
    "        val_matrix=matrix_used[kep_ind, :][:, kep_ind],\n",
    "        graph=graph_used[kep_ind, :][:, kep_ind],\n",
    "        var_names=caus_tigr[i]['var_names'][kep_ind],\n",
    "        cmap_edges='RdBu_r', vmin_edges=-max_cross, vmax_edges=max_cross, edge_ticks=max_cross*2/5,\n",
    "        cmap_nodes='OrRd', vmin_nodes=0, vmax_nodes=max_auto, node_ticks=max_auto/5,\n",
    "        node_label_size=20,# int, optional (default: 10)\n",
    "        link_label_fontsize=20,# : int, optional (default: 6)    \n",
    "        fig_ax = (fig, axes[i_c]), show_colorbar=False,\n",
    "        )\n",
    "    \n",
    "plt.subplots_adjust(left=0, bottom=0.1, right=1, top=.97, wspace=0.1, hspace=0.1) \n",
    "fig.savefig(f'{output_loc}Causality_{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "del(fig, axes, c_map_ax1, ticks_auto, cbar1, c_map_ax2, ticks_cross, cbar2, i_c, i, kep_ind, matrix_used,\n",
    "    graph_used, j, caus_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=1)\n",
    "\n",
    "for i in pat_names_short[:]:\n",
    "    # keep only the climatic indices that are causaly connected with the Mediterranean pattern of interest\n",
    "    kep_ind = (caus_tigr[i]['graph']=='')[:, 0, :].sum(axis=(1))\n",
    "    kep_ind = kep_ind != (t_max-t_min+2)\n",
    "    \n",
    "    # remove all cross links that are not from the indicators to the pattern\n",
    "    matrix_used = caus_tigr[i]['val_matrix'].copy()\n",
    "    graph_used = caus_tigr[i]['graph'].copy()\n",
    "    for j in range(1, graph_used.shape[0]):\n",
    "        graph_used[np.delete(np.arange(graph_used.shape[0]), j), j, :] = ''\n",
    "    \n",
    "    caus_plot = tp.plot_graph(\n",
    "        val_matrix=matrix_used[kep_ind, :][:, kep_ind],\n",
    "        graph=graph_used[kep_ind, :][:, kep_ind],\n",
    "        var_names=caus_tigr[i]['var_names'][kep_ind],\n",
    "        link_colorbar_label='cross-MCI', vmin_edges=-max_cross, vmax_edges=max_cross, edge_ticks=max_cross*2/5,\n",
    "        node_colorbar_label='auto-MCI', vmin_nodes=0, vmax_nodes=max_auto, node_ticks=max_auto/5,\n",
    "        figsize=(4, 4),\n",
    "        save_name=f'{output_loc}Causality_{i}_{temp_aggr}{temp_resolution}.png'\n",
    "        )\n",
    "    \n",
    "    if i!=pat_names_short[-1]: plt.close()\n",
    "        \n",
    "del(i, kep_ind, matrix_used, graph_used, j, caus_plot, max_cross, max_auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilinear with Feature Selection (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_incl_pattern(input_data):\n",
    "    \n",
    "    type_used, ind_used, shift_used, cl_used = input_data\n",
    "    \n",
    "    ts_pat_df = ts_patterns_final.sel(Type=type_used, Indicator=ind_used, cluster=cl_used).to_dataframe('Pat')\n",
    "    \n",
    "    ts_ind_df = ts_indices_final.sel(Type=type_used, index=indices_used_feat_sel)\n",
    "    ts_ind_df = ts_ind_df.assign_coords({'index': [indices_used_final_names[i] for i in ts_ind_df.index.values]})\n",
    "    ts_ind_df = ts_ind_df.to_dataframe('type_used')\n",
    "    ts_ind_df = ts_ind_df.pivot_table(index='time', columns='index', values='type_used')\n",
    "    \n",
    "    ts_ind_df = pd.concat([ts_pat_df[['Pat']], ts_ind_df], axis=1) # use also the actual pattern timeseries\n",
    "\n",
    "    ts_ind_df = ts_ind_df.shift(shift_used).dropna()\n",
    "\n",
    "    ts_ind_df = ts_ind_df[ts_ind_df.index.month.isin(months_analysed)]\n",
    "    ts_pat_df = ts_pat_df.loc[ts_ind_df.index]\n",
    "    \n",
    "#     feat_sel_test = mutual_info_regression(ts_ind_df, ts_pat_df[['Pat']])\n",
    "    feat_sel_test = f_regression(ts_ind_df, ts_pat_df[['Pat']])[0]\n",
    "    feat_sel_test = np.abs(feat_sel_test) # make absolute because sometimes negative values occur\n",
    "    feat_sel_test /= np.max(feat_sel_test)\n",
    "    \n",
    "    feat_sel = pd.DataFrame({'Feat_sel_test': feat_sel_test}, index=ts_ind_df.columns)\n",
    "    feat_sel = feat_sel.sort_values('Feat_sel_test', ascending=False)\n",
    "\n",
    "    final_ind_used = feat_sel.index[:7] # keep always at least 7 instances\n",
    "    if len(feat_sel.query('Feat_sel_test>0.05').index)>7:\n",
    "        final_ind_used = feat_sel.query('Feat_sel_test>0.05').index\n",
    "    final_ind_used = final_ind_used[:10] # no more than 10 instances (in case too many the Feat_sel_test>0.05)\n",
    "    c = [comb for i in range(len(final_ind_used)) for comb in combinations(final_ind_used, i + 1)]\n",
    "    \n",
    "    ols_col_names = ['const', 'Pat']+list(indices_used_final_names.values())\n",
    "    ols_col_names += ['corr', 'corr_adj', 'n_features', 'pattern_used']\n",
    "    ols_results = pd.DataFrame(columns=ols_col_names, index=range(len(c)))\n",
    "    for i_index, i_c in enumerate(c):\n",
    "        res_comb = OLS(ts_pat_df['Pat'], add_constant(ts_ind_df[list(i_c)])).fit()    \n",
    "        ols_results.loc[i_index, ['const']+list(i_c)] = res_comb.params\n",
    "        ols_results.loc[i_index, 'corr'] = res_comb.rsquared**.5\n",
    "        ols_results.loc[i_index, 'corr_adj'] = res_comb.rsquared_adj**.5\n",
    "        ols_results.loc[i_index, 'n_features'] = len(i_c)\n",
    "        ols_results.loc[i_index, 'pattern_used'] = 'Pat' in list(i_c)\n",
    "\n",
    "    ols_results = ols_results.astype('float')\n",
    "    ols_results = xr.DataArray(ols_results).rename({'dim_0': 'combination', 'dim_1': 'ols_param'})\n",
    "    ols_results = ols_results.assign_coords({'cluster': cl_used})\n",
    "    ols_results = ols_results.assign_coords({'lag': -shift_used, 'Type': type_used, 'Indicator': ind_used})\n",
    "\n",
    "    ols_results = ols_results.expand_dims(['lag', 'Type', 'Indicator', 'cluster']) # for xr.combine_by_coords\n",
    "    \n",
    "    return ols_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = list(product(ts_patterns_final.Type.values, ts_patterns_final.Indicator.values, \n",
    "                     range(ols_min, ols_max), ts_patterns_final.cluster.values))\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "multivar_corr_feat_sel_all = list(tqdm.tqdm(pool.imap(ols_incl_pattern, combs), total=len(combs), position=0))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "multivar_corr_feat_sel_all = xr.combine_by_coords(multivar_corr_feat_sel_all)\n",
    "file_name = f'{output_loc}MultiVarCorrelationsIndicesFeatSel_InclPat{temp_aggr}{temp_resolution}.nc'\n",
    "multivar_corr_feat_sel_all.to_netcdf(file_name)\n",
    "\n",
    "del(combs, pool, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_corr(input_data):\n",
    "    \n",
    "    multi_corr_used, type_used, ind_used, lag_used, cl_used = input_data\n",
    "    \n",
    "    data = multi_corr_used.sel(lag=lag_used, Type=type_used, Indicator=ind_used, cluster=cl_used)\n",
    "\n",
    "    data = data.assign_coords({'combination': data.sel(ols_param='n_features').values})\n",
    "    \n",
    "    # nans cause problem, so change them to 0\n",
    "    combs_modified = [i if ~np.isnan(i) else 0 for i in data.combination.values]\n",
    "    data = data.assign_coords({'combination': combs_modified})\n",
    "    \n",
    "#     reslt = data.sel(ols_param='corr_adj').groupby('combination').max()\n",
    "    \n",
    "    data_corr_adj = data.sel(ols_param='corr_adj').fillna(-999) # again convert nan, otherwise function breaks\n",
    "\n",
    "    reslt = data_corr_adj.groupby('combination').apply(lambda c: c.argmax(dim='combination'))\n",
    "    reslt = [(i[1].isel(combination=j.values)) for i, j in zip(data.groupby('combination'), reslt)]\n",
    "    reslt = xr.concat(reslt, dim='combination')\n",
    "    \n",
    "    reslt_aux = (reslt.isel(combination=0)*0).assign_coords({'combination': -1}) # in case only comb=0 exists\n",
    "    reslt = xr.concat([reslt_aux, reslt], dim='combination')\n",
    "\n",
    "    reslt = reslt.isel(combination=reslt.combination!=0) # NaN's combs are changed to 0, so should be removed\n",
    "    reslt = reslt.where(reslt != -999) # convert the -999 back to nan\n",
    "    reslt = reslt.assign_coords({'lag': lag_used, 'Type': type_used, 'Indicator': ind_used, 'cluster': cl_used})\n",
    "\n",
    "    reslt = reslt.expand_dims(['lag', 'Type', 'Indicator', 'cluster']) # for xr.combine_by_coords\n",
    "\n",
    "    return reslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corr_feat_sel = []\n",
    "\n",
    "test_full = multivar_corr_feat_sel_all\n",
    "test_no_param = multivar_corr_feat_sel_all.where(multivar_corr_feat_sel_all.sel(ols_param='pattern_used')==0)\n",
    "test_param = multivar_corr_feat_sel_all.where(multivar_corr_feat_sel_all.sel(ols_param='pattern_used')==1)\n",
    "\n",
    "for test, i_name in zip([test_full, test_no_param, test_param], ['All', 'NoSame', 'YesSame']):\n",
    "    \n",
    "    combs = list(product([test], multivar_corr_feat_sel_all.Type.values,\n",
    "                     multivar_corr_feat_sel_all.Indicator.values, \n",
    "                     multivar_corr_feat_sel_all.lag.values, multivar_corr_feat_sel_all.cluster.values))\n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    max_corr_feat_sel_i = list(tqdm.tqdm(pool.imap(max_corr, combs), total=len(combs), position=0))\n",
    "    pool.close(); pool.join()\n",
    "\n",
    "    max_corr_feat_sel_i = xr.combine_by_coords(max_corr_feat_sel_i)\n",
    "    max_corr_feat_sel_i = max_corr_feat_sel_i.assign_coords({'Subset': i_name})\n",
    "    max_corr_feat_sel.append(max_corr_feat_sel_i)\n",
    "\n",
    "max_corr_feat_sel = xr.concat(max_corr_feat_sel, dim='Subset')\n",
    "\n",
    "# drop the auxilary combination=-1\n",
    "max_corr_feat_sel = max_corr_feat_sel.isel(combination=max_corr_feat_sel.combination>0)\n",
    "\n",
    "del(test_full, test_no_param, test_param, test, i_name, combs, max_corr_feat_sel_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=2)\n",
    "\n",
    "data_plot = max_corr_feat_sel.sel(Type=main_type_used, Indicator=main_indicator_used, \n",
    "                                  cluster=pat_names_short, ols_param='corr_adj', Subset='All')\n",
    "data_plot = data_plot.to_dataframe('Corr').reset_index()\n",
    "data_plot.combination = data_plot.combination.astype(int)\n",
    "data_plot = data_plot.query('combination<=11 and lag<0')\n",
    "data_plot = data_plot.rename(columns={'combination': 'Predictors used', 'cluster': 'Pattern'})\n",
    "data_plot = sns.relplot(data=data_plot, x='lag', y='Corr', kind='line', hue='Predictors used',   \n",
    "                        col='Pattern', col_wrap=3, \n",
    "                        palette=sns.color_palette(n_colors=len(data_plot['Predictors used'].unique())))\n",
    "\n",
    "data_plot.fig.savefig(f'{output_loc}MultiCorrelationsIndices_{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "del(data_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=2)\n",
    "\n",
    "data_plot = max_corr_feat_sel.sel(Type=main_type_used, Indicator=main_indicator_used, \n",
    "                                  cluster=pat_names_short, ols_param='corr_adj', Subset='All')\n",
    "data_plot_diff = data_plot.diff('combination').assign_coords({'combination': data_plot.combination.values[1:]-1})\n",
    "data_plot = data_plot_diff/data_plot*100\n",
    "data_plot = data_plot.assign_coords({'combination': data_plot.combination.values+1})\n",
    "data_plot = data_plot.to_dataframe('Improvement (%)').reset_index()\n",
    "data_plot = data_plot.query('combination<=11 and lag<0')\n",
    "data_plot = data_plot.rename(columns={'combination': 'Predictors used', 'cluster': 'Pattern'})\n",
    "data_plot = sns.relplot(data=data_plot, x='Predictors used', y='Improvement (%)', kind='line', hue='lag', \n",
    "                        col='Pattern',  \n",
    "                        col_wrap=3, palette=sns.color_palette(n_colors=len(data_plot.lag.unique())))\n",
    "\n",
    "data_plot.fig.savefig(f'{output_loc}MultiCorrelationsIndicesPlotElbow_{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "del(data_plot, data_plot_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale=1)\n",
    "\n",
    "used_combs = np.arange(1,7)\n",
    "used_par = max_corr_feat_sel.sel(Type=main_type_used, Indicator=main_indicator_used, lag=[-5, -4, -3, -2, -1], \n",
    "                                  cluster=pat_names_short[:], combination=used_combs, Subset='All')\n",
    "used_par = used_par.dropna(dim='ols_param', how='all')\n",
    "used_par = (~np.isnan(used_par)).sum('combination')\n",
    "used_par = used_par.where(used_par!=0)/len(used_combs)\n",
    "used_par = used_par.to_dataframe('S').reset_index()\n",
    "used_par = used_par.query(\"ols_param not in ['const', 'corr', 'corr_adj', 'n_features', 'pattern_used']\")\n",
    "used_par = used_par.rename(columns={'ols_param': 'Predictor', 'cluster': 'Pattern'})\n",
    "\n",
    "def draw_heatmap(*args, **kwargs):\n",
    "    data = kwargs.pop('data')\n",
    "    d = data.pivot(index=args[1], columns=args[0], values=args[2])\n",
    "    ax = sns.heatmap(d, cmap='YlOrRd', vmax=1, vmin=0, cbar=False, annot=True, fmt='.1f')\n",
    "    ax.set_yticks(np.arange(len(d.index))+.5)\n",
    "    ax.set_yticklabels(d.index)\n",
    "\n",
    "fg = sns.FacetGrid(used_par, col='Pattern', col_wrap=3)\n",
    "fg.fig.set_size_inches(12, 18)\n",
    "fg.map_dataframe(draw_heatmap, 'lag', 'Predictor', 'S')\n",
    "fg.fig.savefig(f'{output_loc}MultiCorrelationsIndicesImportance_{temp_aggr}{temp_resolution}.png', dpi=600)\n",
    "del(fg, draw_heatmap, used_par, used_combs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
