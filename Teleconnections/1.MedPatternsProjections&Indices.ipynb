{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xmca.xarray import xMCA\n",
    "from xeofs.xarray import EOF\n",
    "\n",
    "import xskillscore as xs\n",
    "from scipy.signal import detrend\n",
    "\n",
    "from itertools import product\n",
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid # for multiplots and nice mixing with cartopy\n",
    "from cartopy.mpl.geoaxes import GeoAxes # for adding cartopy attributes to subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = ''\n",
    "\n",
    "# data for Mediterranean patterns (based on previous work)\n",
    "patterns_file = 'Med_LocalizedPatterns_Composites.nc'\n",
    "clusters_for_patterns = 9\n",
    "rolling_days = 1 # rolling days used for the Med. Patterns\n",
    "use_standardised_anom = False # Standardized anomalies? If not (default) then only anomalies from Mean value\n",
    "atm_var_used = ['SLP', 'Z500']\n",
    "\n",
    "variables_eof_used = ['SST', 'SMmean', 'SML1', 'SML2'] # variables used for the eof and k-means analysis\n",
    "# area_subset_eof = [60, -20, 15, 60] # boundary used for the eof analysis of the variables used for the indices\n",
    "area_subset_eof = [50, -15, 25, 45] # boundary used for the eof analysis of the variables used for the indices\n",
    "\n",
    "days_shift = 4 # for climatology (use dates before/after date of interest for more robust results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type_name = 'Anomalies_Std' if use_standardised_anom==True else 'Anomalies' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_var_data = []\n",
    "for i_var in atm_var_used:\n",
    "    # read daily data of the atmospheric variable of interest\n",
    "    file_path = dir_loc + 'D1_Mean_'+i_var+'_Anomalies.nc'\n",
    "    daily_data = xr.open_dataarray(file_path).reset_coords(drop=True) # read data\n",
    "    atm_var_data.append(daily_data)\n",
    "    \n",
    "atm_var_data = xr.merge(atm_var_data)\n",
    "\n",
    "del(i_var, file_path, daily_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = xr.open_dataset(patterns_file)\n",
    "patterns = patterns.sel(ClustersNumber=clusters_for_patterns, rolling=rolling_days)\n",
    "patterns = patterns.sel(variable_type=var_type_name).reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "composites = patterns.to_array()\n",
    "subplot_id = list(map(chr, range(97, 123)))# letters for subplots titles\n",
    "colors_used = sns.color_palette('RdBu_r', n_colors=15)\n",
    "colors_used = colors_used[:5]+['white']+colors_used[-5:]\n",
    "colors_used = ListedColormap(colors_used)\n",
    "colors_limits = [-16, -13, -10, -7, -4, -1, 1, 4, 7, 10, 13, 16] # levels for colors (actual abs. max is 15.5)\n",
    "cont_levels = np.linspace(-21, 21, 13) # act. max is 21.2\n",
    "\n",
    "x, y = np.meshgrid(composites.longitude, composites.latitude)\n",
    "max_values = np.abs(composites).max(dim=['cluster', 'latitude', 'longitude']).values\n",
    "max_values = np.round(max_values/np.array([100, 98.1]), 2)\n",
    "print('Absolute Max values for SLP and Z500 are {} hPa and {} dam respectively'.\\\n",
    "      format(max_values[0], max_values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loc = ''\n",
    "\n",
    "pat_names = ['Atlantic Low', 'Biscay Low', 'Iberian Low', 'Sicilian Low', 'Balkan Low', 'Black Sea Low',\n",
    "             'Mediterranean High', 'Minor Low', 'Minor High'] # naming of patterns\n",
    "pat_names_short = ['AtlL', 'BscL', 'IbrL', 'SclL', 'BlkL', 'BlSL', 'MedH', 'MnrL', 'MnrH'] # short names \n",
    "\n",
    "axes_class = (GeoAxes, dict(map_projection=ccrs.PlateCarree()))\n",
    "fig = plt.figure(figsize=(18/2.54, 9.1/2.54))\n",
    "grid = AxesGrid(fig, 111, nrows_ncols=(3,3), axes_pad=.2,\n",
    "                cbar_mode='single', cbar_location='right', cbar_pad=.1,\n",
    "                axes_class=axes_class, cbar_size='3%', label_mode='')\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "\n",
    "    ax.set_extent([x.min(), x.max(), y.min(), y.max()], crs=ccrs.PlateCarree()) # set extent\n",
    "    ax.outline_patch.set_linewidth(.2) # reduce the border thickness\n",
    "    \n",
    "    ax.coastlines(resolution='110m', linewidth=.5, color='grey') # add coastline\n",
    "\n",
    "    contf = ax.contourf(x, y, composites.sel(variable='SLP', cluster=i)/100, # plot contourf for SLP anomalies\n",
    "                        transform=ccrs.PlateCarree(), levels=colors_limits, cmap=colors_used) \n",
    "    cont = ax.contour(x, y, composites.sel(variable='Z500', cluster=i)/98.1, # plot contour for Z500 anomalies\n",
    "                      transform=ccrs.PlateCarree(),levels=cont_levels, colors='black', linewidths=1) \n",
    "    ax.clabel(cont, inline=1, fontsize=6, fmt='%d')\n",
    "    for line, lvl in zip(cont.collections, cont.levels):\n",
    "        if lvl == 0:\n",
    "            line.set_linestyle(':')\n",
    "\n",
    "    ax.set_title('{}) {} ({})'.format(subplot_id[i], pat_names[i], pat_names_short[i]), \n",
    "                 pad=4, size=7.5, loc='left',)# backgroundcolor=pat_colors[i])\n",
    "\n",
    "cbar = ax.cax.colorbar(contf, ticks=colors_limits, spacing='proportional') # add colorbar\n",
    "cbar.ax.set_title(\"SLP' (hPa)\", size=6.5, loc='left')\n",
    "cbar.ax.yaxis.set_tick_params(width=.25, length=2, labelsize=6.5)\n",
    "[i.set_linewidth(0.5) for i in cbar.ax.spines.values()]\n",
    "    \n",
    "plt.subplots_adjust(left=0.02, bottom=0.01, right=.95, top=.96)\n",
    "fig.savefig(f'{output_loc}/Patterns.png', dpi=600, transparent=True)\n",
    "\n",
    "del(axes_class, fig, grid, i, ax, contf, cont, line, lvl, cbar,\n",
    "    output_loc, pat_names, pat_names_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_anom(input_data):\n",
    "    \n",
    "    daymonth_used, standarization = input_data\n",
    "    \n",
    "    # keep dates of interest (exact day month)\n",
    "    dates_used = all_dates_atm_var_extd[all_dates_atm_var_extd.strftime('%m%d').isin([daymonth_used])]\n",
    "    \n",
    "    # add buffer days (before/after) for having a more robust climatology for mean and std\n",
    "    # when considering n-days data (n>1), then use only dates with no overlapping subsets\n",
    "    all_dates_used = [list(pd.date_range(i_dt, i_dt-pd.DateOffset(days=days_shift), freq='-1D'))+\n",
    "                      list(pd.date_range(i_dt, i_dt+pd.DateOffset(days=days_shift), freq='D')[1:]) \n",
    "                      for i_dt in dates_used]\n",
    "    all_dates_used = np.array([j for i in all_dates_used for j in i]) # flatten data to have 1-d array\n",
    "    all_dates_used = all_dates_used[pd.to_datetime(all_dates_used).isin(all_dates_atm_var)] # only existing dates\n",
    "    \n",
    "    # keep all dates of interest and get climatology\n",
    "    subset_i_date = data_used.sel(time=all_dates_used)\n",
    "    subset_i_date_clim = subset_i_date.sortby('time').sel(time=slice('1979','2019')) # same years as Papers 1,2,3\n",
    "    \n",
    "    clim_mean = subset_i_date_clim.mean('time')\n",
    "    \n",
    "    # get (standardized) anomalies for the exact days of interest\n",
    "    subset_used = subset_i_date.sel(time=dates_used[dates_used.isin(all_dates_atm_var)])\n",
    "    anom_final = subset_used-clim_mean\n",
    "    \n",
    "    if standarization:\n",
    "        anom_final = anom_final/subset_i_date_clim.std('time')\n",
    "    \n",
    "    return anom_final.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_var_pat = atm_var_data.sel(longitude=patterns.longitude.values, \n",
    "                               latitude=patterns.latitude.values,\n",
    "                               Type='Actual').drop('Type')\n",
    "atm_var_pat = atm_var_pat[list(patterns.keys())].copy(deep=True)\n",
    "weights = np.cos(np.deg2rad(atm_var_pat.latitude)) # weights due to areal differences of each grid\n",
    "weights_2d = weights.expand_dims({'longitude': atm_var_pat.longitude.values}) # weights on both lat-lon\n",
    "\n",
    "proj_act = (atm_var_pat*patterns).weighted(weights).mean(['latitude', 'longitude']) # projections to patt\n",
    "\n",
    "proj_pat = proj_act/proj_act.std('time') # standardize dot product\n",
    "proj_pat = proj_pat.to_array('atm_var').mean('atm_var') # get mean projection considering all used variables\n",
    "\n",
    "model = EOF(proj_pat, norm=True, dim='time')\n",
    "model.solve()\n",
    "expvar = model.explained_variance_ratio()\n",
    "n_eofs = expvar>.01 # at least 1% of total variance\n",
    "eofs = model.eofs()\n",
    "pcs = model.pcs()\n",
    "pcs = pcs.isel(mode=n_eofs).assign_coords({'mode': np.arange(-sum(n_eofs), 0, 1)}).rename({'mode': 'cluster'})\n",
    "eofs = eofs.isel(mode=n_eofs).assign_coords({'mode': np.arange(-sum(n_eofs), 0, 1)})\n",
    "\n",
    "proj_pat = xr.concat([pcs, proj_pat], dim='cluster')\n",
    "\n",
    "# get the variables used for the clim_anom function\n",
    "all_dates_atm_var = pd.to_datetime(proj_act.time.values)\n",
    "all_dates_atm_var_extd = pd.date_range(all_dates_atm_var[0] - pd.DateOffset(years=1), \n",
    "                                       all_dates_atm_var[-1] + pd.DateOffset(years=1))\n",
    "data_used = proj_act\n",
    "unique_daymonth = all_dates_atm_var.strftime('%m%d')\n",
    "unique_daymonth = sorted(set(unique_daymonth))\n",
    "combs_used = list(product(unique_daymonth, [True]))\n",
    "\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "proj_pat_norm = list(tqdm.tqdm(pool.imap(clim_anom, combs_used), total=len(unique_daymonth), position=0))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "proj_pat_norm = xr.concat(proj_pat_norm, dim='time').sortby('time')\n",
    "proj_pat_norm = proj_pat_norm.to_array('atm_var').mean('atm_var') # mean proj. considering all used variables\n",
    "\n",
    "model = EOF(proj_pat_norm, norm=True, dim='time')\n",
    "model.solve()\n",
    "expvar = model.explained_variance_ratio()\n",
    "n_eofs = expvar>.01 # at least 1% of total variance\n",
    "eofs = model.eofs()\n",
    "pcs = model.pcs()\n",
    "pcs = pcs.isel(mode=n_eofs).assign_coords({'mode': np.arange(-sum(n_eofs), 0, 1)}).rename({'mode': 'cluster'})\n",
    "eofs = eofs.isel(mode=n_eofs).assign_coords({'mode': np.arange(-sum(n_eofs), 0, 1)})\n",
    "\n",
    "proj_pat_norm = xr.concat([pcs, proj_pat_norm], dim='cluster')\n",
    "\n",
    "dim_name = pd.Index(['projection', 'projection_norm', 'correlation'], name='Indicator')[:2]\n",
    "ts_patterns = xr.concat([proj_pat, proj_pat_norm], dim=dim_name) # combine\n",
    "\n",
    "# corr_pat = xs.pearson_r(atm_var_pat, patterns, dim=['latitude', 'longitude'], weights=weights_2d) # cor to pat\n",
    "# ts_patterns = xr.concat([proj_pat, proj_pat_norm, corr_pat], dim=dim_name) # combine\n",
    "# ts_patterns = ts_patterns.to_array().mean('variable')\n",
    "\n",
    "ts_patterns = ts_patterns.transpose('time', ...)\n",
    "\n",
    "del(atm_var_pat, weights, weights_2d, proj_act, proj_pat, model, expvar, n_eofs, eofs, pcs, \n",
    "    all_dates_atm_var, all_dates_atm_var_extd, data_used, unique_daymonth, proj_pat_norm, dim_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_pat_detr = ts_patterns*0+np.apply_along_axis(detrend, 0, ts_patterns)\n",
    "ts_pat_seas = ts_pat_detr.assign_coords({'time': pd.PeriodIndex(ts_pat_detr.time.values, freq='Q-Nov')})\n",
    "ts_pat_seas = ts_pat_seas.groupby('time') - ts_pat_seas.groupby('time').mean()\n",
    "ts_pat_seas = ts_pat_seas.assign_coords({'time': ts_pat_detr.time.values})\n",
    "\n",
    "ts_patterns = [ts_patterns, ts_pat_detr, ts_pat_seas]\n",
    "ts_patterns = xr.concat(ts_patterns, dim=pd.Index(['Actual', 'Detrended', 'InterAnnRemov'], name='Type'))\n",
    "ts_patterns.to_netcdf(dir_loc+'MedIndices.nc')\n",
    "\n",
    "del(ts_pat_detr, ts_pat_seas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices timeseries aquisition and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof_indices = {i_var: {} for i_var in variables_eof_used}\n",
    "for i_var in variables_eof_used:\n",
    "        \n",
    "    print(f'Indices generation for {i_var} started.')\n",
    "    # read daily data of the atmospheric variable of interest\n",
    "    if i_var=='SMmean':\n",
    "        file_path = dir_loc + '../Data/D1_Mean_'+i_var+'.nc'\n",
    "        daily_data = xr.open_dataarray(file_path).reset_coords(drop=True) # read data\n",
    "    else:\n",
    "        file_path = dir_loc + '../Data/D1_Mean_'+i_var+'.grb'\n",
    "        daily_data = xr.open_dataarray(file_path, engine='cfgrib').reset_coords(drop=True) # read data\n",
    "    \n",
    "    daily_data = daily_data.sel(longitude=slice(area_subset_eof[1], area_subset_eof[3]), \n",
    "                                latitude=slice(area_subset_eof[0], area_subset_eof[2]))\n",
    "    daily_data = daily_data.rename({'longitude': 'lon', 'latitude': 'lat'}) # xMCA works with lon/lat\n",
    "\n",
    "    if i_var!='SST':\n",
    "        mask_used = (daily_data.mean('time')<=0) # this is needed cause sea points (SM<=0) should be masked\n",
    "        daily_data = daily_data.where(mask_used==0)\n",
    "        stand_data = False\n",
    "    else:\n",
    "        stand_data = True    \n",
    "        \n",
    "    # mask data so NaNs don't consume memory\n",
    "    mask_used = np.isnan(daily_data.isel(time=0)).reset_coords(drop=True)\n",
    "    daily_data = daily_data.fillna(0)\n",
    "    \n",
    "    # get the variables used for the clim_anom function\n",
    "    all_dates_atm_var = pd.to_datetime(daily_data.time.values)\n",
    "    all_dates_atm_var_extd = pd.date_range(all_dates_atm_var[0] - pd.DateOffset(years=1), \n",
    "                                           all_dates_atm_var[-1] + pd.DateOffset(years=1))\n",
    "    data_used = daily_data\n",
    "    unique_daymonth = all_dates_atm_var.strftime('%m%d')\n",
    "    unique_daymonth = sorted(set(unique_daymonth))\n",
    "    combs_used = list(product(unique_daymonth, [stand_data]))\n",
    "\n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    daily_data = list(tqdm.tqdm(pool.imap(clim_anom, combs_used), total=len(unique_daymonth), position=0))\n",
    "    pool.close(); pool.join()\n",
    "\n",
    "    daily_data = xr.concat(daily_data, dim='time').sortby('time')\n",
    "    daily_data = daily_data.where(mask_used==0) # convert back to the actual NaNs \n",
    "    \n",
    "    pca = xMCA(daily_data)\n",
    "    pca.apply_coslat() # apply weights based on latitude\n",
    "    \n",
    "    # calculate non-complex PCA\n",
    "    pca.solve(complexify=False) # no complex PCA\n",
    "    expvar = pca.explained_variance() # explained variance \n",
    "    pcs = pca.pcs() # Principal component scores (PCs) \n",
    "    eofs = pca.eofs() # spatial patterns (EOFs)\n",
    "\n",
    "    n_eofs = int(np.searchsorted(np.cumsum(expvar), 95)) # n. of EOFs for explaining >= 95% of total variance\n",
    "\n",
    "    # get results after varimax rotation\n",
    "    pca.rotate(n_rot=n_eofs, power=1) # Varimax rotation\n",
    "    expvar_rot = pca.explained_variance() # explained variance \n",
    "    pcs_rot = pca.pcs() # Principal component scores (PCs) \n",
    "    eofs_rot = pca.eofs() # spatial patterns (EOFs)\n",
    "\n",
    "    # keep PCs that have at least 0.9% explained variance\n",
    "    expvar_rot = expvar_rot.isel(mode=expvar_rot>.90)\n",
    "    pcs_rot = pcs_rot['left'].isel(mode=range(len(expvar_rot)))\n",
    "    pcs_rot = pcs_rot.assign_coords({'mode': [f'{i_var}_{i}' for i in range(1, len(pcs_rot.mode)+1)]})\n",
    "    eofs_rot = eofs_rot['left'].isel(mode=range(len(expvar_rot)))\n",
    "    \n",
    "    eof_indices[i_var] = {'ExpVars': expvar_rot, 'PCs': pcs_rot, 'EOFs': eofs_rot}\n",
    "    print(f'Indices generation for {i_var} completed.\\n')\n",
    "    \n",
    "del(i_var, stand_data, file_path, daily_data, all_dates_atm_var, all_dates_atm_var_extd, data_used, \n",
    "    unique_daymonth, combs_used, pool, pca, expvar, pcs, eofs, n_eofs, expvar_rot, pcs_rot, eofs_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "explvar_final = [eof_indices[i_var]['ExpVars'] for i_var in variables_eof_used]\n",
    "explvar_final = xr.concat(explvar_final, dim=pd.Index(variables_eof_used, name='atm_var'))\n",
    "explvar_final.to_netcdf(dir_loc+'ExpVarAtmVars.nc')\n",
    "\n",
    "eofs_final = [eof_indices[i_var]['EOFs'] for i_var in variables_eof_used]\n",
    "eofs_final = xr.concat(eofs_final, dim=pd.Index(variables_eof_used, name='atm_var'))\n",
    "eofs_final.to_netcdf(dir_loc+'EOFAtmVars.nc')\n",
    "\n",
    "pcs_final = [eof_indices[i_var]['PCs'].to_dataframe('S').pivot_table(index='time', columns='mode', values='S')\n",
    "             for i_var in variables_eof_used]\n",
    "pcs_final = pd.concat(pcs_final, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "explvar_final = xr.open_dataarray(dir_loc+'ExpVarAtmVars.nc').sel(atm_var=['SST', 'SML1'])\n",
    "eof_final = xr.open_dataarray(dir_loc+'EOFAtmVars.nc').sel(atm_var=['SST', 'SML1'])\n",
    "used_combs = list(product(np.arange(1, 7), ['SST', 'SML1']))\n",
    "max_val = np.ceil(np.abs(eof_final.sel(mode=range(1, 7))).max()*20)/20\n",
    "colors_used = sns.color_palette('RdBu_r', n_colors=15)\n",
    "colors_used = colors_used[:5]+colors_used[-5:]\n",
    "colors_used = ListedColormap(colors_used)\n",
    "colors_limits = list(np.linspace(-max_val, max_val, 11))\n",
    "x, y = np.meshgrid(eof_final.lon, eof_final.lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_loc = ''\n",
    "\n",
    "axes_class = (GeoAxes, dict(map_projection=ccrs.PlateCarree()))\n",
    "fig = plt.figure(figsize=(8/2.54, 10/2.54))\n",
    "grid = AxesGrid(fig, 111, nrows_ncols=(6,2), axes_pad=.2,\n",
    "                cbar_mode='single', cbar_location='right', cbar_pad=.1,\n",
    "                axes_class=axes_class, cbar_size='3%', label_mode='')\n",
    "\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "\n",
    "    ax.set_extent([x.min(), x.max(), y.min(), y.max()], crs=ccrs.PlateCarree()) # set extent\n",
    "    ax.outline_patch.set_linewidth(.2) # reduce the border thickness\n",
    "    \n",
    "    ax.coastlines(resolution='110m', linewidth=.5, color='grey') # add coastline\n",
    "\n",
    "    contf = ax.contourf(x, y, eof_final.sel(mode=used_combs[i][0], atm_var=used_combs[i][1]),\n",
    "                        transform=ccrs.PlateCarree(), levels=colors_limits, cmap=colors_used)\n",
    "    \n",
    "    ax.set_title('{}-{} ({:.2f}%)'.format(used_combs[i][1][:3], used_combs[i][0], \n",
    "                                     explvar_final.sel(mode=used_combs[i][0], atm_var=used_combs[i][1]).values), \n",
    "                 pad=4, size=7.5, loc='left',)# backgroundcolor=pat_colors[i])\n",
    "\n",
    "cbar = ax.cax.colorbar(contf, ticks=colors_limits, spacing='proportional') # add colorbar\n",
    "cbar.ax.yaxis.set_tick_params(width=.25, length=2, labelsize=6.5)\n",
    "    \n",
    "plt.subplots_adjust(left=0.02, bottom=0.01, right=.85, top=.96)\n",
    "fig.savefig(f'{output_loc}/EOFs.png', dpi=600, transparent=True)\n",
    "\n",
    "del(fig, cbar, ax, contf, i, grid, axes_class, output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noaa_indices(indice_name, suffix='b500101'):\n",
    "    \n",
    "    indice_data = f'ftp://ftp.cpc.ncep.noaa.gov/cwlinks/norm.daily.{indice_name}.index.{suffix}.current.ascii'\n",
    "    indice_data = pd.read_csv(indice_data, delimiter=\"\\s+\", header=None)\n",
    "    indice_data.columns = ['Year', 'Month', 'Day', indice_name.upper()]\n",
    "    indice_data['Day'] = indice_data['Day'].str.replace(r'*', '').astype(int)\n",
    "    indice_data.index = pd.to_datetime(indice_data[['Year', 'Month', 'Day']])\n",
    "    indice_data = indice_data[[indice_name.upper()]]\n",
    "\n",
    "    return indice_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo_indices(mo_name):\n",
    "    \n",
    "    'data from https://crudata.uea.ac.uk/cru/data/moi/'\n",
    "    # mo_name is either \"moi1\" (Alriers/Cairo) or \"moi2\" (Gibraltar/Israel)\n",
    "    \n",
    "    ind_mo = f'/{mo_name}.output.dat.txt'\n",
    "    ind_mo = pd.read_csv(ind_mo, delimiter=\".\", header=None, index_col=0,)\n",
    "    ind_mo.columns = ['Month', 'Day', mo_name.upper(), 'Add']\n",
    "    ind_mo[mo_name.upper()] = ind_mo[mo_name.upper()].astype(str)+'.'+ind_mo['Add'].astype(str)\n",
    "    ind_mo = ind_mo.astype('float')\n",
    "    ind_mo['Year'] = ind_mo.index\n",
    "    \n",
    "    ind_mo.index = pd.to_datetime(ind_mo[['Year', 'Month', 'Day']])\n",
    "    ind_mo = ind_mo[[mo_name.upper()]]\n",
    "\n",
    "    return ind_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate West Mediterranean Oscillation, similar as https://doi.org/10.1002/joc.1388\n",
    "\n",
    "slp_all = 'D1_Mean_SLP.grb'\n",
    "slp_all = xr.open_dataarray(slp_all, engine='cfgrib').reset_coords(drop=True)\n",
    "\n",
    "slp_Padua = slp_all.sel(latitude=45.24, longitude=11.47, method=\"nearest\")\n",
    "slp_Padua = slp_Padua.reset_coords(drop=True).to_dataframe('Padua')\n",
    "slp_Cadiz = slp_all.sel(latitude=36.17, longitude=-6.07, method=\"nearest\")\n",
    "slp_Cadiz = slp_Cadiz.reset_coords(drop=True).to_dataframe('Cadiz')\n",
    "\n",
    "west_MO = pd.concat([slp_Cadiz, slp_Padua], axis=1)\n",
    "means = west_MO.groupby([west_MO.index.month]).transform('mean')\n",
    "stds = west_MO.groupby([west_MO.index.month]).transform('std')\n",
    "west_MO = (west_MO-means)/stds\n",
    "west_MO = west_MO['Cadiz'] - west_MO['Padua']\n",
    "west_MO = pd.DataFrame(west_MO, columns=['WeMO'])\n",
    "\n",
    "del(slp_all, slp_Padua, slp_Cadiz, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_BSISO = pd.read_pickle(dir_loc+'../Data/daily_total_precipitation_anomaly_EOF_norm_projections')\n",
    "ind_BSISO.index = pd.to_datetime(ind_BSISO.index)\n",
    "ind_BSISO.columns = ['BSISO_EOF1', 'BSISO_EOF2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjo_BOM = pd.read_csv(dir_loc+'../Data/rmm.txt', delimiter=\"\\s+\", skiprows=[0])\n",
    "\n",
    "mjo_BOM = mjo_BOM.iloc[:, :7] # keep only the actual MJO_BOM of interest\n",
    "mjo_BOM.columns = ['Year', 'Month', 'Day', 'MJO_RMM1', 'MJO_RMM2', 'MJO_Phase', 'MJO_Amplitude'] # rename columns\n",
    "mjo_BOM.index = pd.to_datetime(mjo_BOM[['Year', 'Month', 'Day']]) # create datetime column\n",
    "\n",
    "# change missing value flags with NaN (Missing Value= 1.E36 or 999)\n",
    "mjo_BOM[mjo_BOM==999] = np.nan\n",
    "mjo_BOM.loc[mjo_BOM.MJO_RMM1>1e+35, 'MJO_RMM1'] = np.nan\n",
    "mjo_BOM.loc[mjo_BOM.MJO_RMM2>1e+35, 'MJO_RMM2'] = np.nan\n",
    "mjo_BOM.loc[mjo_BOM.MJO_Amplitude>1e+35, 'MJO_Amplitude'] = np.nan\n",
    "\n",
    "mjo_BOM = mjo_BOM[['MJO_RMM1', 'MJO_RMM2', 'MJO_Amplitude', 'MJO_Phase']] # columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sahel precipitation daily index, considering same area as http://research.jisao.washington.edu/data/sahel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prpShl = xr.open_dataarray(dir_loc+'../Data/D1_Total_Precipitation.grb', engine='cfgrib').reset_coords(drop=True)\n",
    "prpShl = prpShl.sel(latitude=slice(20, 10), longitude=slice(-10, 20))\n",
    "weights = np.cos(np.deg2rad(prpShl.latitude))\n",
    "prpShl = prpShl.weighted(weights).mean(['latitude', 'longitude'])\n",
    "prpShl = prpShl.assign_coords({'time': pd.to_datetime(prpShl.time)-np.timedelta64(18, 'h')})\n",
    "del(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates_atm_var = pd.to_datetime(prpShl.time.values)\n",
    "all_dates_atm_var_extd = pd.date_range(all_dates_atm_var[0] - pd.DateOffset(years=1), \n",
    "                                       all_dates_atm_var[-1] + pd.DateOffset(years=1))\n",
    "data_used = prpShl\n",
    "unique_daymonth = all_dates_atm_var.strftime('%m%d')\n",
    "unique_daymonth = sorted(set(unique_daymonth))\n",
    "combs_used = list(product(unique_daymonth, [True]))\n",
    "\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "prpShl_norm = list(tqdm.tqdm(pool.imap(clim_anom, combs_used), total=len(unique_daymonth), position=0))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "prpShl_norm = xr.concat(prpShl_norm, dim='time').sortby('time')\n",
    "prpShl_norm = prpShl_norm.to_dataframe('PrecipSahel')\n",
    "\n",
    "del(all_dates_atm_var, all_dates_atm_var_extd, data_used, unique_daymonth, combs_used, pool, prpShl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate indices for sea ice area fraction over parts of Northern Hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_ice = xr.open_dataarray(dir_loc+'../Data/D1_Mean_SIAF.grb', engine='cfgrib').reset_coords(drop=True)\n",
    "\n",
    "areas = {'ArcticWhole': [slice(90, 70), slice(-180, 180)],\n",
    "         'ArcticEurope': [slice(90, 70), slice(-30, 60)],\n",
    "         'ArcticEurasia': [slice(90, 70), slice(-30, 180)]}\n",
    "\n",
    "sea_ice_indices = []\n",
    "for i_area, i_domain in areas.items():\n",
    "    i_sea_ice = sea_ice.sel(latitude=i_domain[0], longitude=i_domain[1])\n",
    "    weights = np.cos(np.deg2rad(i_sea_ice.latitude))\n",
    "    i_sea_ice = i_sea_ice.weighted(weights).mean(['latitude', 'longitude'])\n",
    "    i_sea_ice.name = i_area\n",
    "    sea_ice_indices.append(i_sea_ice)\n",
    "    \n",
    "sea_ice_indices = xr.merge(sea_ice_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates_atm_var = pd.to_datetime(sea_ice_indices.time.values)\n",
    "all_dates_atm_var_extd = pd.date_range(all_dates_atm_var[0] - pd.DateOffset(years=1), \n",
    "                                       all_dates_atm_var[-1] + pd.DateOffset(years=1))\n",
    "data_used = sea_ice_indices\n",
    "unique_daymonth = all_dates_atm_var.strftime('%m%d')\n",
    "unique_daymonth = sorted(set(unique_daymonth))\n",
    "combs_used = list(product(unique_daymonth, [True]))\n",
    "\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "sea_ice_norm = list(tqdm.tqdm(pool.imap(clim_anom, combs_used), total=len(unique_daymonth), position=0))\n",
    "pool.close(); pool.join()\n",
    "\n",
    "sea_ice_norm = xr.concat(sea_ice_norm, dim='time').sortby('time')\n",
    "sea_ice_norm = sea_ice_norm.to_dataframe()\n",
    "\n",
    "del(all_dates_atm_var,all_dates_atm_var_extd, data_used, unique_daymonth, sea_ice, areas, i_area, i_domain,\n",
    "    combs_used, pool, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_Linus = !ls *csv\n",
    "\n",
    "# ts_ind_linus = []\n",
    "\n",
    "# for i_ind in indices_Linus:\n",
    "#     ts_i = pd.read_csv(i_ind, index_col=0)\n",
    "#     ts_i.index = pd.to_datetime(ts_i.index)\n",
    "#     prefix_name = i_ind.replace('replace', '').replace('.csv', '') # replace section of names\n",
    "#     ts_i.columns = [f'Linus_{prefix_name}_{i}' for i in ts_i.columns]\n",
    "#     ts_ind_linus.append(ts_i)\n",
    "    \n",
    "# ts_ind_linus = pd.concat(ts_ind_linus, axis=1).dropna()\n",
    "\n",
    "# del(indices_Linus, i_ind, ts_i, prefix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_indices = pd.concat([noaa_indices('nao'), noaa_indices('pna'),\n",
    "                        noaa_indices('ao'), # noaa_indices('aao', 'b790101'), # aao file is mising lately...\n",
    "                        mo_indices('moi1'), mo_indices('moi2'), west_MO, \n",
    "                        ind_BSISO, mjo_BOM, prpShl_norm, sea_ice_norm, pcs_final, ts_ind_linus], \n",
    "                       axis=1).dropna()\n",
    "\n",
    "del(west_MO, ind_BSISO, ts_ind_linus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_indices.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_indices = xr.DataArray(ts_indices).rename({'dim_0': 'time', 'dim_1': 'index'})\n",
    "ts_ind_detr = ts_indices*0+np.apply_along_axis(detrend, 0, ts_indices)\n",
    "ts_ind_seas = ts_ind_detr.assign_coords({'time': pd.PeriodIndex(ts_ind_detr.time.values, freq='Q-Nov')})\n",
    "ts_ind_seas = ts_ind_seas.groupby('time') - ts_ind_seas.groupby('time').mean()\n",
    "ts_ind_seas = ts_ind_seas.assign_coords({'time': ts_ind_detr.time.values})\n",
    "\n",
    "ts_indices = [ts_indices, ts_ind_detr, ts_ind_seas]\n",
    "ts_indices = xr.concat(ts_indices, dim=pd.Index(['Actual', 'Detrended', 'InterAnnRemov'], name='Type'))\n",
    "ts_indices.to_netcdf(dir_loc+'AtmIndices.nc')\n",
    "\n",
    "del(ts_ind_detr, ts_ind_seas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
